#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è©³ç´°çš„æ—¥æ–‡ç¶²ç«™èª¿è©¦è…³æœ¬
åˆ†æç·¨ç¢¼å•é¡Œå’Œ HTML çµæ§‹è®ŠåŒ–
"""

import httpx
import chardet
from bs4 import BeautifulSoup
from urllib.parse import quote
import sys
import time

def test_encoding_methods(content_bytes, url):
    """æ¸¬è©¦ä¸åŒç·¨ç¢¼æ–¹æ³•"""
    print(f"\nğŸ” æ¸¬è©¦ä¸åŒç·¨ç¢¼æ–¹æ³• - {url}")
    print("=" * 60)
    
    encodings_to_test = ['utf-8', 'cp932', 'shift_jis', 'euc-jp', 'iso-2022-jp']
    
    # 1. chardet è‡ªå‹•æª¢æ¸¬
    detected = chardet.detect(content_bytes)
    print(f"ğŸ“Š chardet æª¢æ¸¬çµæœ: {detected}")
    
    # 2. æ¸¬è©¦å„ç¨®ç·¨ç¢¼
    for encoding in encodings_to_test:
        try:
            decoded_text = content_bytes.decode(encoding, errors='replace')
            replacement_ratio = decoded_text.count('ï¿½') / len(decoded_text) if decoded_text else 1.0
            print(f"  {encoding:12}: æ›¿æ›æ¯”ä¾‹ {replacement_ratio:.3f}")
            
            # å¦‚æœæ›¿æ›æ¯”ä¾‹å¾ˆä½ï¼Œé¡¯ç¤ºå‰200å­—ç¬¦
            if replacement_ratio < 0.1:
                preview = decoded_text[:200].replace('\n', ' ').replace('\r', ' ')
                print(f"    é è¦½: {preview}...")
                
        except (UnicodeDecodeError, LookupError) as e:
            print(f"  {encoding:12}: è§£ç¢¼å¤±æ•— - {e}")
    
    return detected.get('encoding', 'utf-8') if detected else 'utf-8'

def analyze_html_structure(soup, site_name):
    """åˆ†æ HTML çµæ§‹"""
    print(f"\nğŸ—ï¸ åˆ†æ {site_name} HTML çµæ§‹")
    print("=" * 60)
    
    # åŸºæœ¬çµ±è¨ˆ
    print(f"ğŸ“„ ç¸½æ–‡å­—é•·åº¦: {len(soup.get_text())}")
    print(f"ğŸ·ï¸  ç¸½æ¨™ç±¤æ•¸é‡: {len(soup.find_all())}")
    
    # æœå°‹ç›¸é—œçš„é¡åˆ¥å’ŒID
    classes_to_check = [
        'actress-name', 'product-div', 'search-result', 'result',
        'product', 'item', 'card', 'entry', 'post', 'content'
    ]
    
    print(f"\nğŸ” æª¢æŸ¥å¸¸è¦‹é¡åˆ¥:")
    for class_name in classes_to_check:
        elements = soup.find_all(class_=class_name)
        if elements:
            print(f"  âœ… .{class_name}: æ‰¾åˆ° {len(elements)} å€‹å…ƒç´ ")
            # é¡¯ç¤ºç¬¬ä¸€å€‹å…ƒç´ çš„éƒ¨åˆ†å…§å®¹
            first_text = elements[0].get_text()[:100].strip().replace('\n', ' ')
            if first_text:
                print(f"      å…§å®¹ç¯„ä¾‹: {first_text}...")
        else:
            print(f"  âŒ .{class_name}: æœªæ‰¾åˆ°")
    
    # æª¢æŸ¥å¯èƒ½çš„å¥³å„ªåç¨±æ¨¡å¼
    print(f"\nğŸ‘© å°‹æ‰¾å¯èƒ½çš„å¥³å„ªåç¨±:")
    potential_actress_patterns = [
        soup.find_all('a', href=lambda x: x and 'actress' in x.lower()),
        soup.find_all('span', class_=lambda x: x and 'name' in x.lower()),
        soup.find_all('div', class_=lambda x: x and 'performer' in x.lower()),
        soup.find_all(text=lambda x: x and any(char in x for char in 'å¥³å„ªæ¼”å“¡å‡ºæ¼”è€…'))
    ]
    
    for i, pattern_results in enumerate(potential_actress_patterns):
        if pattern_results:
            print(f"  æ¨¡å¼ {i+1}: æ‰¾åˆ° {len(pattern_results)} å€‹å¯èƒ½çµæœ")
            for result in pattern_results[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                if hasattr(result, 'get_text'):
                    text = result.get_text().strip()
                else:
                    text = str(result).strip()
                if text:
                    print(f"    - {text[:50]}...")

def test_japanese_sites():
    """æ¸¬è©¦æ—¥æ–‡ç¶²ç«™çš„å¯¦éš›å›æ‡‰"""
    test_codes = ['SONE-553', 'FWAY-031']
    
    sites = [
        {
            'name': 'AV-WIKI',
            'url_template': 'https://av-wiki.net/?s={}&post_type=product',
            'expected_classes': ['actress-name']
        },
        {
            'name': 'chiba-f.net',  
            'url_template': 'https://chiba-f.net/search/?keyword={}',
            'expected_classes': ['product-div', 'fw-bold']
        }
    ]
    
    with httpx.Client(timeout=30) as client:
        for site in sites:
            print(f"\n{'='*80}")
            print(f"ğŸŒ æ¸¬è©¦ç¶²ç«™: {site['name']}")
            print(f"{'='*80}")
            
            for code in test_codes:
                print(f"\nğŸ” æ¸¬è©¦ç•ªè™Ÿ: {code}")
                url = site['url_template'].format(quote(code))
                print(f"ğŸ“‹ è«‹æ±‚ URL: {url}")
                
                try:
                    # ç™¼é€è«‹æ±‚
                    response = client.get(url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1'
                    })
                    
                    print(f"ğŸ“Š å›æ‡‰ç‹€æ…‹: {response.status_code}")
                    print(f"ğŸ“ å…§å®¹é•·åº¦: {len(response.content)} bytes")
                    
                    if response.status_code == 200:
                        # åˆ†æç·¨ç¢¼
                        best_encoding = test_encoding_methods(response.content, url)
                        
                        # å˜—è©¦ç”¨æœ€ä½³ç·¨ç¢¼è§£æ
                        try:
                            soup = BeautifulSoup(response.content, 'html.parser', from_encoding=best_encoding)
                            analyze_html_structure(soup, site['name'])
                            
                            # æª¢æŸ¥æœŸæœ›çš„é¡åˆ¥
                            print(f"\nğŸ¯ æª¢æŸ¥æœŸæœ›çš„é¡åˆ¥:")
                            for expected_class in site['expected_classes']:
                                elements = soup.find_all(class_=expected_class)
                                print(f"  .{expected_class}: {len(elements)} å€‹å…ƒç´ ")
                                
                        except Exception as e:
                            print(f"âŒ HTML è§£æå¤±æ•—: {e}")
                    else:
                        print(f"âŒ è«‹æ±‚å¤±æ•—: {response.status_code}")
                        print(f"ğŸ’¬ å›æ‡‰å…§å®¹: {response.text[:200]}...")
                        
                except Exception as e:
                    print(f"âŒ è«‹æ±‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                print(f"\nâ° ç­‰å¾… 2 ç§’...")
                time.sleep(2)

if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹•æ—¥æ–‡ç¶²ç«™è©³ç´°èª¿è©¦")
    print("æ­¤è…³æœ¬å°‡åˆ†ææ—¥æ–‡ç¶²ç«™çš„ç·¨ç¢¼å’Œ HTML çµæ§‹å•é¡Œ")
    print("=" * 80)
    
    test_japanese_sites()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ èª¿è©¦å®Œæˆï¼è«‹æª¢æŸ¥ä»¥ä¸Šè¼¸å‡ºä¾†è¨ºæ–·å•é¡Œã€‚")
