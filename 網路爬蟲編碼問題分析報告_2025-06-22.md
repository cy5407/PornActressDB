# 網路爬蟲編碼問題分析報告
## 日期：2025-06-22

### 🔍 問題摘要
您在日誌中看到的編碼警告確實指向了重要的網路爬蟲問題。經過深入分析，我們發現以下核心問題：

### 📊 測試結果
| 網站 | 狀態 | 最佳編碼 | 替換字符數 | 有意義內容 |
|------|------|----------|-------------|------------|
| av-wiki.net | 200 OK | cp932 | 1025+ | ❌ 無 |
| chiba-f.net | 200 OK | cp932 | 287+ | ❌ 無 |

### ⚠️ 發現的問題

#### 1. 編碼問題
- **替換字符過多**：即使使用最佳編碼 `cp932`，仍有大量 `�` 替換字符
- **內容無法解析**：頁面標題顯示為「無標題」
- **結構識別失敗**：無法找到搜尋結果容器

#### 2. 可能原因
1. **JavaScript 動態載入**：內容可能通過 AJAX 動態載入
2. **反爬蟲機制**：網站返回加密或混淆的內容
3. **特殊編碼**：使用了非標準字符編碼
4. **請求標頭要求**：需要特定的 Cookie 或標頭

### 💡 解決方案建議

#### 1. 短期解決方案（立即可實作）

##### A. 改進編碼處理
```python
class ImprovedEncodingHandler:
    def __init__(self):
        self.encoding_priority = ['cp932', 'shift_jis', 'utf-8', 'euc-jp']
    
    def smart_decode(self, content: bytes) -> str:
        for encoding in self.encoding_priority:
            try:
                decoded = content.decode(encoding, errors='replace')
                replacement_ratio = decoded.count('�') / len(decoded)
                
                # 如果替換字符比例小於 5%，認為編碼正確
                if replacement_ratio < 0.05:
                    return decoded, encoding
            except:
                continue
        
        # 回退到 UTF-8
        return content.decode('utf-8', errors='replace'), 'utf-8'
```

##### B. 增強請求標頭
```python
ENHANCED_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate, br',
    'Referer': 'https://www.google.com/',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}
```

##### C. 請求間隔和重試
```python
import time
import random
from functools import wraps

def rate_limit(min_delay=1, max_delay=3):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            time.sleep(random.uniform(min_delay, max_delay))
            return func(*args, **kwargs)
        return wrapper
    return decorator
```

#### 2. 中期解決方案（需要額外套件）

##### A. 使用 Selenium 進行瀏覽器自動化
```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def create_chrome_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    driver = webdriver.Chrome(options=options)
    return driver

def scrape_with_selenium(url: str):
    driver = create_chrome_driver()
    try:
        driver.get(url)
        time.sleep(3)  # 等待 JavaScript 載入
        return driver.page_source
    finally:
        driver.quit()
```

##### B. 使用 Playwright（推薦）
```python
from playwright.sync_api import sync_playwright

def scrape_with_playwright(url: str):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        
        page.goto(url)
        page.wait_for_load_state('networkidle')
        content = page.content()
        
        browser.close()
        return content
```

#### 3. 長期解決方案（架構改進）

##### A. 多重爬蟲策略
```python
class MultiScrapingStrategy:
    def __init__(self):
        self.strategies = [
            self.basic_requests,
            self.selenium_scraping,
            self.playwright_scraping
        ]
    
    def scrape(self, url: str):
        for strategy in self.strategies:
            try:
                result = strategy(url)
                if self.is_valid_content(result):
                    return result
            except Exception as e:
                logger.warning(f"Strategy failed: {e}")
                continue
        
        raise Exception("All scraping strategies failed")
```

##### B. 智慧快取系統
```python
import hashlib
import pickle
from pathlib import Path

class SmartCache:
    def __init__(self, cache_dir='cache', ttl=3600):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.ttl = ttl
    
    def get_cache_key(self, url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()
    
    def get(self, url: str):
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.pkl"
        if cache_file.exists():
            mtime = cache_file.stat().st_mtime
            if time.time() - mtime < self.ttl:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
        return None
    
    def set(self, url: str, data):
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
```

### 🎯 實作優先順序

#### 第一階段（立即實作）
1. ✅ 改進編碼檢測和處理邏輯
2. ✅ 添加更真實的瀏覽器標頭
3. ✅ 實作請求間隔和重試機制
4. ✅ 加強錯誤處理和日誌記錄

#### 第二階段（1週內）
1. 🔄 測試和整合 Selenium 或 Playwright
2. 🔄 實作智慧快取系統
3. 🔄 建立爬蟲效能監控
4. 🔄 添加代理伺服器支援

#### 第三階段（2週內）
1. 📝 完整的爬蟲重構
2. 📝 API 端點分析和整合
3. 📝 多重爬蟲策略實作
4. 📝 自動化測試套件

### 📋 推薦的檔案結構
```
src/scrapers/
├── enhanced/
│   ├── encoding_handler.py      # 改進的編碼處理
│   ├── browser_automation.py    # 瀏覽器自動化
│   ├── smart_cache.py          # 智慧快取系統
│   └── multi_strategy.py       # 多重爬蟲策略
├── utils/
│   ├── rate_limiter.py         # 頻率限制
│   ├── proxy_manager.py        # 代理管理
│   └── monitoring.py           # 效能監控
└── tests/
    ├── test_encoding.py         # 編碼測試
    └── test_scrapers.py         # 爬蟲測試
```

### 🔧 立即可執行的修復

基於分析結果，我建議您：

1. **立即更新現有爬蟲模組**的編碼處理邏輯
2. **添加更強健的錯誤處理**，避免因編碼問題導致系統崩潰
3. **實作請求間隔**，避免被網站封鎖
4. **考慮使用 Selenium 或 Playwright** 作為備用方案

這些改進將顯著減少您看到的編碼警告，並提高爬蟲的成功率和穩定性。

---
**分析完成時間**：2025-06-22  
**建議實作順序**：編碼處理 → 瀏覽器自動化 → 快取系統 → 多重策略
