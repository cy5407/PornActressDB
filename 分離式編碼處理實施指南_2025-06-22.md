# å¥³å„ªåˆ†é¡ç³»çµ±ç·¨ç¢¼å•é¡Œåˆ†é›¢è™•ç†æŒ‡å—
## æ—¥æœŸï¼š2025-06-22

### ğŸ¯ ç›®æ¨™
å›é€€åˆ° JAVDB æ­£å¸¸å·¥ä½œçš„ç‰ˆæœ¬ï¼Œç„¶å¾Œé‡æ–°å¯¦æ–½åˆ†é›¢å¼ç·¨ç¢¼è™•ç†ï¼Œç¢ºä¿ï¼š
1. æ—¥æ–‡ç¶²ç«™ï¼ˆav-wiki.netã€chiba-f.netï¼‰ä½¿ç”¨ CP932 ç·¨ç¢¼è™•ç†
2. JAVDB ä¿æŒåŸæœ‰çš„ UTF-8 æ¨™æº–è™•ç†
3. ä¸åŒç¶²ç«™ä½¿ç”¨ä¸åŒçš„è«‹æ±‚é–“éš”ç­–ç•¥

### ğŸ“‹ åŸ·è¡Œæ­¥é©Ÿ

#### ç¬¬ä¸€éšæ®µï¼šå›é€€åˆ°ç©©å®šç‰ˆæœ¬

1. **æŸ¥çœ‹ Git æ­·å²**
```bash
cd "c:\Users\cy540\OneDrive\æ¡Œé¢\Python\å¥³å„ªåˆ†é¡_é‡æ§‹20250617\å¥³å„ªåˆ†é¡"
git log --oneline -10
```

2. **æ‰¾åˆ° JAVDB æ­£å¸¸å·¥ä½œçš„ç‰ˆæœ¬**
å°‹æ‰¾ç·¨ç¢¼ä¿®æ­£å‰çš„æäº¤ï¼Œé€šå¸¸æ˜¯ `ab648ae` æˆ–é¡ä¼¼çš„æäº¤ã€‚

3. **å›é€€åˆ°ç©©å®šç‰ˆæœ¬**
```bash
git reset --hard ab648ae
```

4. **ç¢ºèªå›é€€æˆåŠŸ**
```bash
git status
git log --oneline -3
```

#### ç¬¬äºŒéšæ®µï¼šæª¢æŸ¥ä¸¦æ¸…ç†æª”æ¡ˆ

1. **æª¢æŸ¥ç¾æœ‰æª”æ¡ˆçµæ§‹**
```bash
ls -la src/services/
```

2. **ç§»é™¤å¯èƒ½è¡çªçš„æª”æ¡ˆ**
```bash
# å¦‚æœå­˜åœ¨ç·¨ç¢¼ç›¸é—œæª”æ¡ˆï¼Œå…ˆå‚™ä»½å†ç§»é™¤
if [ -f "src/services/encoding_enhancer.py" ]; then
    mv src/services/encoding_enhancer.py src/services/encoding_enhancer.py.backup
fi
```

#### ç¬¬ä¸‰éšæ®µï¼šå‰µå»ºæ—¥æ–‡ç¶²ç«™å°ˆç”¨ç·¨ç¢¼è™•ç†å™¨

1. **å‰µå»º `src/services/japanese_site_enhancer.py`**

```python
# -*- coding: utf-8 -*-
"""
æ—¥æ–‡ç¶²ç«™ç·¨ç¢¼å¢å¼·å™¨ - å°ˆé–€è™•ç† av-wiki.net å’Œ chiba-f.net
"""

import logging
from typing import Tuple, Optional
from bs4 import BeautifulSoup
import httpx

logger = logging.getLogger(__name__)

class JapaneseSiteEnhancer:
    """æ—¥æ–‡ç¶²ç«™ç·¨ç¢¼å¢å¼·å™¨ - å°ˆç‚º av-wiki.net å’Œ chiba-f.net è¨­è¨ˆ"""
    
    def __init__(self):
        # æ—¥æ–‡ç¶²ç«™çš„ç·¨ç¢¼å„ªå…ˆé †åºï¼ˆæ ¹æ“šæ¸¬è©¦çµæœå„ªåŒ–ï¼‰
        self.encoding_priority = [
            'cp932',      # æ—¥æ–‡ Windows ç·¨ç¢¼ï¼ˆæ¸¬è©¦æœ€ä½³ï¼‰
            'shift_jis',  # æ—¥æ–‡ç·¨ç¢¼
            'euc-jp',     # æ—¥æ–‡ Unix ç·¨ç¢¼
            'utf-8',      # ä½œç‚ºæœ€å¾Œå‚™é¸
        ]
        
        # æ”¯æ´çš„æ—¥æ–‡ç¶²ç«™åŸŸå
        self.supported_domains = [
            'av-wiki.net',
            'chiba-f.net'
        ]
    
    def is_japanese_site(self, url: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ”¯æ´çš„æ—¥æ–‡ç¶²ç«™"""
        return any(domain in url for domain in self.supported_domains)
    
    def create_enhanced_soup(self, response: httpx.Response, url: str = "") -> BeautifulSoup:
        """
        ç‚ºæ—¥æ–‡ç¶²ç«™å‰µå»ºç¶“éç·¨ç¢¼å„ªåŒ–çš„ BeautifulSoup ç‰©ä»¶
        
        Args:
            response: httpx.Response ç‰©ä»¶
            url: ä¾†æº URL
            
        Returns:
            BeautifulSoup ç‰©ä»¶
        """
        if not self.is_japanese_site(url):
            # å¦‚æœä¸æ˜¯æ—¥æ–‡ç¶²ç«™ï¼Œä½¿ç”¨æ¨™æº–è™•ç†
            logger.debug(f"éæ—¥æ–‡ç¶²ç«™ï¼Œä½¿ç”¨æ¨™æº–ç·¨ç¢¼è™•ç†: {url}")
            return BeautifulSoup(response.content, "html.parser")
        
        content_bytes = response.content
        if not content_bytes:
            return BeautifulSoup("", "html.parser")
        
        best_soup = None
        best_encoding = 'utf-8'
        min_replacement_ratio = float('inf')
        
        for encoding in self.encoding_priority:
            try:
                decoded = content_bytes.decode(encoding, errors='replace')
                
                # è¨ˆç®—æ›¿æ›å­—ç¬¦æ¯”ä¾‹
                replacement_count = decoded.count('\ufffd')
                replacement_ratio = replacement_count / len(decoded) if decoded else 1.0
                
                if replacement_ratio < min_replacement_ratio:
                    min_replacement_ratio = replacement_ratio
                    best_encoding = encoding
                    best_soup = BeautifulSoup(decoded, "html.parser")
                    
                    # å¦‚æœæ›¿æ›å­—ç¬¦å¾ˆå°‘ï¼Œå°±ä½¿ç”¨é€™å€‹ç·¨ç¢¼
                    if replacement_ratio < 0.05:  # 5% ä»¥ä¸‹
                        break
                        
            except (UnicodeDecodeError, LookupError) as e:
                logger.debug(f"ç·¨ç¢¼ {encoding} è§£ç¢¼å¤±æ•—: {e}")
                continue
        
        if best_soup is None:
            # å¦‚æœæ‰€æœ‰ç·¨ç¢¼éƒ½å¤±æ•—ï¼Œä½¿ç”¨æ¨™æº–è™•ç†
            logger.warning(f"æ‰€æœ‰ç·¨ç¢¼å˜—è©¦éƒ½å¤±æ•—ï¼Œä½¿ç”¨æ¨™æº–è™•ç†: {url}")
            return BeautifulSoup(response.content, "html.parser")
        
        # è¨˜éŒ„æˆåŠŸä½¿ç”¨çš„ç·¨ç¢¼
        if min_replacement_ratio > 0.01:  # åªåœ¨æ›¿æ›å­—ç¬¦è¼ƒå¤šæ™‚è¨˜éŒ„è­¦å‘Š
            logger.info(f"[{url}] æ—¥æ–‡ç¶²ç«™ä½¿ç”¨ç·¨ç¢¼: {best_encoding} (æ›¿æ›æ¯”ä¾‹: {min_replacement_ratio:.3f})")
        else:
            logger.debug(f"[{url}] æ—¥æ–‡ç¶²ç«™ä½¿ç”¨ç·¨ç¢¼: {best_encoding} (æ›¿æ›æ¯”ä¾‹: {min_replacement_ratio:.3f})")
            
        return best_soup

# å‰µå»ºå…¨åŸŸå¯¦ä¾‹
japanese_enhancer = JapaneseSiteEnhancer()

# ä¾¿æ·å‡½å¼
def create_japanese_soup(response: httpx.Response, url: str = "") -> BeautifulSoup:
    """ä¾¿æ·å‡½å¼ï¼šç‚ºæ—¥æ–‡ç¶²ç«™å‰µå»ºå¢å¼·çš„ BeautifulSoup"""
    return japanese_enhancer.create_enhanced_soup(response, url)

def is_japanese_site(url: str) -> bool:
    """ä¾¿æ·å‡½å¼ï¼šæª¢æŸ¥æ˜¯å¦ç‚ºæ—¥æ–‡ç¶²ç«™"""
    return japanese_enhancer.is_japanese_site(url)
```

#### ç¬¬å››éšæ®µï¼šä¿®æ”¹ web_searcher.py

1. **ä¿®æ”¹åŒ¯å…¥éƒ¨åˆ†**

åœ¨æª”æ¡ˆé ‚éƒ¨æ·»åŠ ï¼š
```python
from .japanese_site_enhancer import create_japanese_soup, is_japanese_site
```

2. **ä¿®æ”¹åˆå§‹åŒ–æ–¹æ³•**

åœ¨ `__init__` æ–¹æ³•ä¸­ï¼Œæ‰¾åˆ°å®‰å…¨æœå°‹å™¨é…ç½®éƒ¨åˆ†ï¼Œä¿®æ”¹ç‚ºï¼š

```python
    def __init__(self, config: ConfigManager):
        # åˆå§‹åŒ–ä¸€èˆ¬æœå°‹å™¨é…ç½®ï¼ˆç”¨æ–¼ JAVDBï¼‰
        safe_config = RequestConfig(
            min_interval=config.getfloat('search', 'min_interval', fallback=1.0),
            max_interval=config.getfloat('search', 'max_interval', fallback=3.0),
            enable_cache=config.getboolean('search', 'enable_cache', fallback=True),
            cache_duration=config.getint('search', 'cache_duration', fallback=86400),
            max_retries=config.getint('search', 'max_retries', fallback=3),
            backoff_factor=config.getfloat('search', 'backoff_factor', fallback=2.0),
            rotate_headers=config.getboolean('search', 'rotate_headers', fallback=True)
        )
        
        # åˆå§‹åŒ–æ—¥æ–‡ç¶²ç«™å°ˆç”¨é…ç½®ï¼ˆè¼ƒå¿«é€Ÿï¼Œå› ç‚ºæ¯”è¼ƒä¸æœƒæ“‹çˆ¬èŸ²ï¼‰
        japanese_config = RequestConfig(
            min_interval=config.getfloat('search', 'japanese_min_interval', fallback=0.5),
            max_interval=config.getfloat('search', 'japanese_max_interval', fallback=1.5),
            enable_cache=config.getboolean('search', 'enable_cache', fallback=True),
            cache_duration=config.getint('search', 'cache_duration', fallback=86400),
            max_retries=config.getint('search', 'max_retries', fallback=3),
            backoff_factor=config.getfloat('search', 'backoff_factor', fallback=1.5),
            rotate_headers=config.getboolean('search', 'rotate_headers', fallback=True)
        )
        
        # åˆå§‹åŒ–æœå°‹å™¨
        self.safe_searcher = SafeSearcher(safe_config)  # ç”¨æ–¼ä¸€èˆ¬æœå°‹
        self.japanese_searcher = SafeSearcher(japanese_config)  # å°ˆç”¨æ–¼æ—¥æ–‡ç¶²ç«™
        
        # åˆå§‹åŒ– JAVDB å®‰å…¨æœå°‹å™¨ï¼ˆä¿æŒåŸæœ‰è¨­å®šï¼‰
        cache_dir = config.get('search', 'cache_dir', fallback=None)
        self.javdb_searcher = SafeJAVDBSearcher(cache_dir)
        
        # ...existing code...
        
        logger.info("ğŸ›¡ï¸ å·²å•Ÿç”¨å®‰å…¨æœå°‹å™¨åŠŸèƒ½")
        logger.info("ğŸ‡¯ğŸ‡µ å·²å•Ÿç”¨æ—¥æ–‡ç¶²ç«™å¿«é€Ÿæœå°‹åŠŸèƒ½")
        logger.info("ğŸ¬ å·²å•Ÿç”¨ JAVDB å®‰å…¨æœå°‹åŠŸèƒ½")
```

3. **ä¿®æ”¹ `_search_av_wiki` æ–¹æ³•**

æ‰¾åˆ° `_search_av_wiki` æ–¹æ³•ï¼Œä¿®æ”¹è«‹æ±‚éƒ¨åˆ†ï¼š

```python
    def _search_av_wiki(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """AV-WIKI æœå°‹æ–¹æ³• - ä½¿ç”¨æ—¥æ–‡ç·¨ç¢¼å¢å¼·"""
        if stop_event.is_set():
            return None
            
        search_url = f"https://av-wiki.net/?s={quote(code)}&post_type=product"
        
        # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æœå°‹å™¨é€²è¡Œè«‹æ±‚
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                response = client.get(url)
                response.raise_for_status()
                # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™ç·¨ç¢¼å¢å¼·å™¨
                return create_japanese_soup(response, url)
        
        try:
            # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æœå°‹å™¨ï¼ˆè¼ƒçŸ­å»¶é²ï¼‰
            soup = self.japanese_searcher.safe_request(make_request, search_url)
            
            # ...existing code for processing...
            
        except Exception as e:
            logger.error(f"AV-WIKI æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        
        return None
```

4. **ä¿®æ”¹ `_search_chiba_f_net` æ–¹æ³•**

æ‰¾åˆ° `_search_chiba_f_net` æ–¹æ³•ï¼Œé€²è¡Œé¡ä¼¼ä¿®æ”¹ï¼š

```python
    def _search_chiba_f_net(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """ä½¿ç”¨ chiba-f.net æœå°‹å¥³å„ªè³‡è¨Š - ä½¿ç”¨æ—¥æ–‡ç·¨ç¢¼å¢å¼·"""
        if stop_event.is_set():
            return None
            
        search_url = f"https://chiba-f.net/search/?keyword={quote(code)}"
        
        # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æœå°‹å™¨é€²è¡Œè«‹æ±‚
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                response = client.get(url)
                response.raise_for_status()
                # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™ç·¨ç¢¼å¢å¼·å™¨
                return create_japanese_soup(response, url)
        
        try:
            # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æœå°‹å™¨ï¼ˆè¼ƒçŸ­å»¶é²ï¼‰
            soup = self.japanese_searcher.safe_request(make_request, search_url)
            
            # ...existing code for processing...
            
        except Exception as e:
            logger.error(f"chiba-f.net æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            
        return None
```

#### ç¬¬äº”éšæ®µï¼šç¢ºä¿ JAVDB æœå°‹å™¨ä¸è®Š

1. **æª¢æŸ¥ `safe_javdb_searcher.py`**

ç¢ºèªæª”æ¡ˆä¸­æ²’æœ‰åŒ¯å…¥ `encoding_enhancer` æˆ–é¡ä¼¼æ¨¡çµ„ï¼š

```python
# ç¢ºä¿åŒ¯å…¥éƒ¨åˆ†åªæœ‰æ¨™æº–æ¨¡çµ„
import time
import random
import httpx
from bs4 import BeautifulSoup
import logging
# ... å…¶ä»–æ¨™æº–åŒ¯å…¥
```

2. **ç¢ºèªè§£ææ–¹æ³•ä½¿ç”¨æ¨™æº–ç·¨ç¢¼**

ç¢ºä¿æ‰€æœ‰ BeautifulSoup å‰µå»ºéƒ½ä½¿ç”¨æ¨™æº–æ–¹å¼ï¼š

```python
soup = BeautifulSoup(response.text, 'html.parser')
```

#### ç¬¬å…­éšæ®µï¼šæ¸¬è©¦é©—è­‰

1. **å‰µå»ºæ¸¬è©¦è…³æœ¬**

ä¿®æ”¹ `test_encoding_fixes.py`ï¼š

```python
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦åˆ†é›¢å¼ç·¨ç¢¼è™•ç†æ•ˆæœ
"""

import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent / 'å¥³å„ªåˆ†é¡'
sys.path.insert(0, str(project_root / 'src'))

import logging
from models.config import ConfigManager
from services.web_searcher import WebSearcher
import threading

# è¨­å®šè©³ç´°æ—¥èªŒ
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def test_separated_encoding():
    """æ¸¬è©¦åˆ†é›¢å¼ç·¨ç¢¼è™•ç†"""
    
    logger.info("ğŸ§ª æ¸¬è©¦åˆ†é›¢å¼ç·¨ç¢¼è™•ç†...")
    
    try:
        # å»ºç«‹é…ç½®ç®¡ç†å™¨
        config = ConfigManager()
        
        # å»ºç«‹ç¶²è·¯æœå°‹å™¨
        searcher = WebSearcher(config)
        
        # æ¸¬è©¦ç·¨ç¢¼å•é¡Œè¼ƒåš´é‡çš„å½±ç‰‡ç·¨è™Ÿ
        test_codes = ['MIDV-661', 'FSDSS-870']
        
        stop_event = threading.Event()
        
        for code in test_codes:
            logger.info(f"\nğŸ” æ¸¬è©¦æœå°‹: {code}")
            logger.info("-" * 50)
            
            # é€²è¡Œæœå°‹
            result = searcher.search_info(code, stop_event)
            
            if result:
                logger.info(f"âœ… æˆåŠŸæ‰¾åˆ° {code}:")
                logger.info(f"   ä¾†æº: {result.get('source', 'Unknown')}")
                logger.info(f"   å¥³å„ª: {', '.join(result.get('actresses', []))}")
                logger.info(f"   ç‰‡å•†: {result.get('studio', 'Unknown')}")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {code} çš„è³‡è¨Š")
        
        logger.info("\nğŸ“Š æª¢æŸ¥ç·¨ç¢¼è­¦å‘Š...")
        logger.info("å¦‚æœæ²’æœ‰ 'bs4.dammit - WARNING' å‡ºç¾ï¼Œèªªæ˜ç·¨ç¢¼å•é¡Œå·²è§£æ±º")
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {e}", exc_info=True)

if __name__ == "__main__":
    test_separated_encoding()
```

2. **åŸ·è¡Œæ¸¬è©¦**

```bash
cd "c:\Users\cy540\OneDrive\æ¡Œé¢\Python\å¥³å„ªåˆ†é¡_é‡æ§‹20250617"
python test_encoding_fixes.py
```

3. **é©—è­‰çµæœ**

æª¢æŸ¥è¼¸å‡ºä¸­ï¼š
- âŒ **ä¸æ‡‰è©²å‡ºç¾**ï¼š`bs4.dammit - WARNING - Some characters could not be decoded`
- âœ… **æ‡‰è©²å‡ºç¾**ï¼š`æ—¥æ–‡ç¶²ç«™ä½¿ç”¨ç·¨ç¢¼: cp932` æˆ–é¡ä¼¼è¨Šæ¯
- âœ… **JAVDB æœå°‹**ï¼šæ­£å¸¸é‹ä½œï¼Œæ‰¾åˆ°å¥³å„ªè³‡æ–™

#### ç¬¬ä¸ƒéšæ®µï¼šæäº¤è®Šæ›´

1. **æª¢æŸ¥ä¿®æ”¹ç‹€æ…‹**
```bash
git status
git diff
```

2. **æ·»åŠ æ–°æª”æ¡ˆ**
```bash
git add src/services/japanese_site_enhancer.py
git add src/services/web_searcher.py
git add test_encoding_fixes.py
```

3. **æäº¤è®Šæ›´**
```bash
git commit -m "feat: å¯¦æ–½åˆ†é›¢å¼ç·¨ç¢¼è™•ç†ç­–ç•¥

- æ–°å¢æ—¥æ–‡ç¶²ç«™å°ˆç”¨ç·¨ç¢¼å¢å¼·å™¨
- åˆ†é›¢ av-wiki.net/chiba-f.net èˆ‡ JAVDB çš„ç·¨ç¢¼è™•ç†
- ç‚ºæ—¥æ–‡ç¶²ç«™è¨­å®šè¼ƒçŸ­è«‹æ±‚é–“éš”
- ä¿æŒ JAVDB åŸæœ‰æ¨™æº–ç·¨ç¢¼è™•ç†
- è§£æ±ºç·¨ç¢¼è­¦å‘Šå•é¡Œ"
```

### ğŸ¯ é æœŸçµæœ

å®Œæˆå¾Œæ‡‰è©²é”åˆ°ï¼š

1. **ç·¨ç¢¼å•é¡Œè§£æ±º**ï¼š
   - æ—¥æ–‡ç¶²ç«™ç„¡ç·¨ç¢¼è­¦å‘Š
   - CP932 ç·¨ç¢¼æ­£ç¢ºè§£ææ—¥æ–‡å…§å®¹

2. **JAVDB åŠŸèƒ½æ­£å¸¸**ï¼š
   - æœå°‹æˆåŠŸç‡ä¸è®Š
   - ç„¡ç·¨ç¢¼å•é¡Œ

3. **æ•ˆèƒ½å„ªåŒ–**ï¼š
   - æ—¥æ–‡ç¶²ç«™ä½¿ç”¨è¼ƒçŸ­å»¶é²ï¼ˆ0.5-1.5ç§’ï¼‰
   - JAVDB ç¶­æŒåŸæœ‰å»¶é²ç­–ç•¥

4. **ç³»çµ±ç©©å®šæ€§**ï¼š
   - ä¸åŒç¶²ç«™ä½¿ç”¨é©åˆçš„ç·¨ç¢¼ç­–ç•¥
   - é¿å…æ··åˆç·¨ç¢¼å•é¡Œ

### âš ï¸ æ³¨æ„äº‹é …

1. **å‚™ä»½é‡è¦**ï¼šåŸ·è¡Œå‰ç¢ºä¿æœ‰ Git å‚™ä»½
2. **æ¸¬è©¦å……åˆ†**ï¼šæ¯å€‹éšæ®µéƒ½è¦æ¸¬è©¦
3. **æ—¥èªŒç›£æ§**ï¼šæ³¨æ„è§€å¯Ÿç·¨ç¢¼è­¦å‘Šæ˜¯å¦æ¶ˆå¤±
4. **æ•ˆèƒ½é©—è­‰**ï¼šç¢ºèªæœå°‹é€Ÿåº¦å’ŒæˆåŠŸç‡

### ğŸ†˜ å›æ»¾æ–¹æ¡ˆ

å¦‚æœå‡ºç¾å•é¡Œï¼š

```bash
# å›åˆ°ä¿®æ”¹å‰çš„ç‹€æ…‹
git reset --hard HEAD~1

# æˆ–å›åˆ°æŒ‡å®šæäº¤
git reset --hard ab648ae
```

é€™å€‹æŒ‡å—æ‡‰è©²èƒ½è®“ Claude Code è‡ªå‹•åŸ·è¡Œä¸¦å®Œæˆåˆ†é›¢å¼ç·¨ç¢¼è™•ç†çš„å¯¦æ–½ã€‚
