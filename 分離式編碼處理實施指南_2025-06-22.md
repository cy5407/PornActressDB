# 女優分類系統編碼問題分離處理指南
## 日期：2025-06-22

### 🎯 目標
回退到 JAVDB 正常工作的版本，然後重新實施分離式編碼處理，確保：
1. 日文網站（av-wiki.net、chiba-f.net）使用 CP932 編碼處理
2. JAVDB 保持原有的 UTF-8 標準處理
3. 不同網站使用不同的請求間隔策略

### 📋 執行步驟

#### 第一階段：回退到穩定版本

1. **查看 Git 歷史**
```bash
cd "c:\Users\cy540\OneDrive\桌面\Python\女優分類_重構20250617\女優分類"
git log --oneline -10
```

2. **找到 JAVDB 正常工作的版本**
尋找編碼修正前的提交，通常是 `ab648ae` 或類似的提交。

3. **回退到穩定版本**
```bash
git reset --hard ab648ae
```

4. **確認回退成功**
```bash
git status
git log --oneline -3
```

#### 第二階段：檢查並清理檔案

1. **檢查現有檔案結構**
```bash
ls -la src/services/
```

2. **移除可能衝突的檔案**
```bash
# 如果存在編碼相關檔案，先備份再移除
if [ -f "src/services/encoding_enhancer.py" ]; then
    mv src/services/encoding_enhancer.py src/services/encoding_enhancer.py.backup
fi
```

#### 第三階段：創建日文網站專用編碼處理器

1. **創建 `src/services/japanese_site_enhancer.py`**

```python
# -*- coding: utf-8 -*-
"""
日文網站編碼增強器 - 專門處理 av-wiki.net 和 chiba-f.net
"""

import logging
from typing import Tuple, Optional
from bs4 import BeautifulSoup
import httpx

logger = logging.getLogger(__name__)

class JapaneseSiteEnhancer:
    """日文網站編碼增強器 - 專為 av-wiki.net 和 chiba-f.net 設計"""
    
    def __init__(self):
        # 日文網站的編碼優先順序（根據測試結果優化）
        self.encoding_priority = [
            'cp932',      # 日文 Windows 編碼（測試最佳）
            'shift_jis',  # 日文編碼
            'euc-jp',     # 日文 Unix 編碼
            'utf-8',      # 作為最後備選
        ]
        
        # 支援的日文網站域名
        self.supported_domains = [
            'av-wiki.net',
            'chiba-f.net'
        ]
    
    def is_japanese_site(self, url: str) -> bool:
        """檢查是否為支援的日文網站"""
        return any(domain in url for domain in self.supported_domains)
    
    def create_enhanced_soup(self, response: httpx.Response, url: str = "") -> BeautifulSoup:
        """
        為日文網站創建經過編碼優化的 BeautifulSoup 物件
        
        Args:
            response: httpx.Response 物件
            url: 來源 URL
            
        Returns:
            BeautifulSoup 物件
        """
        if not self.is_japanese_site(url):
            # 如果不是日文網站，使用標準處理
            logger.debug(f"非日文網站，使用標準編碼處理: {url}")
            return BeautifulSoup(response.content, "html.parser")
        
        content_bytes = response.content
        if not content_bytes:
            return BeautifulSoup("", "html.parser")
        
        best_soup = None
        best_encoding = 'utf-8'
        min_replacement_ratio = float('inf')
        
        for encoding in self.encoding_priority:
            try:
                decoded = content_bytes.decode(encoding, errors='replace')
                
                # 計算替換字符比例
                replacement_count = decoded.count('\ufffd')
                replacement_ratio = replacement_count / len(decoded) if decoded else 1.0
                
                if replacement_ratio < min_replacement_ratio:
                    min_replacement_ratio = replacement_ratio
                    best_encoding = encoding
                    best_soup = BeautifulSoup(decoded, "html.parser")
                    
                    # 如果替換字符很少，就使用這個編碼
                    if replacement_ratio < 0.05:  # 5% 以下
                        break
                        
            except (UnicodeDecodeError, LookupError) as e:
                logger.debug(f"編碼 {encoding} 解碼失敗: {e}")
                continue
        
        if best_soup is None:
            # 如果所有編碼都失敗，使用標準處理
            logger.warning(f"所有編碼嘗試都失敗，使用標準處理: {url}")
            return BeautifulSoup(response.content, "html.parser")
        
        # 記錄成功使用的編碼
        if min_replacement_ratio > 0.01:  # 只在替換字符較多時記錄警告
            logger.info(f"[{url}] 日文網站使用編碼: {best_encoding} (替換比例: {min_replacement_ratio:.3f})")
        else:
            logger.debug(f"[{url}] 日文網站使用編碼: {best_encoding} (替換比例: {min_replacement_ratio:.3f})")
            
        return best_soup

# 創建全域實例
japanese_enhancer = JapaneseSiteEnhancer()

# 便捷函式
def create_japanese_soup(response: httpx.Response, url: str = "") -> BeautifulSoup:
    """便捷函式：為日文網站創建增強的 BeautifulSoup"""
    return japanese_enhancer.create_enhanced_soup(response, url)

def is_japanese_site(url: str) -> bool:
    """便捷函式：檢查是否為日文網站"""
    return japanese_enhancer.is_japanese_site(url)
```

#### 第四階段：修改 web_searcher.py

1. **修改匯入部分**

在檔案頂部添加：
```python
from .japanese_site_enhancer import create_japanese_soup, is_japanese_site
```

2. **修改初始化方法**

在 `__init__` 方法中，找到安全搜尋器配置部分，修改為：

```python
    def __init__(self, config: ConfigManager):
        # 初始化一般搜尋器配置（用於 JAVDB）
        safe_config = RequestConfig(
            min_interval=config.getfloat('search', 'min_interval', fallback=1.0),
            max_interval=config.getfloat('search', 'max_interval', fallback=3.0),
            enable_cache=config.getboolean('search', 'enable_cache', fallback=True),
            cache_duration=config.getint('search', 'cache_duration', fallback=86400),
            max_retries=config.getint('search', 'max_retries', fallback=3),
            backoff_factor=config.getfloat('search', 'backoff_factor', fallback=2.0),
            rotate_headers=config.getboolean('search', 'rotate_headers', fallback=True)
        )
        
        # 初始化日文網站專用配置（較快速，因為比較不會擋爬蟲）
        japanese_config = RequestConfig(
            min_interval=config.getfloat('search', 'japanese_min_interval', fallback=0.5),
            max_interval=config.getfloat('search', 'japanese_max_interval', fallback=1.5),
            enable_cache=config.getboolean('search', 'enable_cache', fallback=True),
            cache_duration=config.getint('search', 'cache_duration', fallback=86400),
            max_retries=config.getint('search', 'max_retries', fallback=3),
            backoff_factor=config.getfloat('search', 'backoff_factor', fallback=1.5),
            rotate_headers=config.getboolean('search', 'rotate_headers', fallback=True)
        )
        
        # 初始化搜尋器
        self.safe_searcher = SafeSearcher(safe_config)  # 用於一般搜尋
        self.japanese_searcher = SafeSearcher(japanese_config)  # 專用於日文網站
        
        # 初始化 JAVDB 安全搜尋器（保持原有設定）
        cache_dir = config.get('search', 'cache_dir', fallback=None)
        self.javdb_searcher = SafeJAVDBSearcher(cache_dir)
        
        # ...existing code...
        
        logger.info("🛡️ 已啟用安全搜尋器功能")
        logger.info("🇯🇵 已啟用日文網站快速搜尋功能")
        logger.info("🎬 已啟用 JAVDB 安全搜尋功能")
```

3. **修改 `_search_av_wiki` 方法**

找到 `_search_av_wiki` 方法，修改請求部分：

```python
    def _search_av_wiki(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """AV-WIKI 搜尋方法 - 使用日文編碼增強"""
        if stop_event.is_set():
            return None
            
        search_url = f"https://av-wiki.net/?s={quote(code)}&post_type=product"
        
        # 使用日文網站專用搜尋器進行請求
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                response = client.get(url)
                response.raise_for_status()
                # 使用日文網站編碼增強器
                return create_japanese_soup(response, url)
        
        try:
            # 使用日文網站專用搜尋器（較短延遲）
            soup = self.japanese_searcher.safe_request(make_request, search_url)
            
            # ...existing code for processing...
            
        except Exception as e:
            logger.error(f"AV-WIKI 搜尋 {code} 時發生錯誤: {e}", exc_info=True)
        
        return None
```

4. **修改 `_search_chiba_f_net` 方法**

找到 `_search_chiba_f_net` 方法，進行類似修改：

```python
    def _search_chiba_f_net(self, code: str, stop_event: threading.Event) -> Optional[Dict]:
        """使用 chiba-f.net 搜尋女優資訊 - 使用日文編碼增強"""
        if stop_event.is_set():
            return None
            
        search_url = f"https://chiba-f.net/search/?keyword={quote(code)}"
        
        # 使用日文網站專用搜尋器進行請求
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                response = client.get(url)
                response.raise_for_status()
                # 使用日文網站編碼增強器
                return create_japanese_soup(response, url)
        
        try:
            # 使用日文網站專用搜尋器（較短延遲）
            soup = self.japanese_searcher.safe_request(make_request, search_url)
            
            # ...existing code for processing...
            
        except Exception as e:
            logger.error(f"chiba-f.net 搜尋 {code} 時發生錯誤: {e}", exc_info=True)
            
        return None
```

#### 第五階段：確保 JAVDB 搜尋器不變

1. **檢查 `safe_javdb_searcher.py`**

確認檔案中沒有匯入 `encoding_enhancer` 或類似模組：

```python
# 確保匯入部分只有標準模組
import time
import random
import httpx
from bs4 import BeautifulSoup
import logging
# ... 其他標準匯入
```

2. **確認解析方法使用標準編碼**

確保所有 BeautifulSoup 創建都使用標準方式：

```python
soup = BeautifulSoup(response.text, 'html.parser')
```

#### 第六階段：測試驗證

1. **創建測試腳本**

修改 `test_encoding_fixes.py`：

```python
# -*- coding: utf-8 -*-
"""
測試分離式編碼處理效果
"""

import sys
from pathlib import Path

# 添加專案路徑
project_root = Path(__file__).parent / '女優分類'
sys.path.insert(0, str(project_root / 'src'))

import logging
from models.config import ConfigManager
from services.web_searcher import WebSearcher
import threading

# 設定詳細日誌
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def test_separated_encoding():
    """測試分離式編碼處理"""
    
    logger.info("🧪 測試分離式編碼處理...")
    
    try:
        # 建立配置管理器
        config = ConfigManager()
        
        # 建立網路搜尋器
        searcher = WebSearcher(config)
        
        # 測試編碼問題較嚴重的影片編號
        test_codes = ['MIDV-661', 'FSDSS-870']
        
        stop_event = threading.Event()
        
        for code in test_codes:
            logger.info(f"\n🔍 測試搜尋: {code}")
            logger.info("-" * 50)
            
            # 進行搜尋
            result = searcher.search_info(code, stop_event)
            
            if result:
                logger.info(f"✅ 成功找到 {code}:")
                logger.info(f"   來源: {result.get('source', 'Unknown')}")
                logger.info(f"   女優: {', '.join(result.get('actresses', []))}")
                logger.info(f"   片商: {result.get('studio', 'Unknown')}")
            else:
                logger.warning(f"⚠️ 未找到 {code} 的資訊")
        
        logger.info("\n📊 檢查編碼警告...")
        logger.info("如果沒有 'bs4.dammit - WARNING' 出現，說明編碼問題已解決")
        
    except Exception as e:
        logger.error(f"❌ 測試失敗: {e}", exc_info=True)

if __name__ == "__main__":
    test_separated_encoding()
```

2. **執行測試**

```bash
cd "c:\Users\cy540\OneDrive\桌面\Python\女優分類_重構20250617"
python test_encoding_fixes.py
```

3. **驗證結果**

檢查輸出中：
- ❌ **不應該出現**：`bs4.dammit - WARNING - Some characters could not be decoded`
- ✅ **應該出現**：`日文網站使用編碼: cp932` 或類似訊息
- ✅ **JAVDB 搜尋**：正常運作，找到女優資料

#### 第七階段：提交變更

1. **檢查修改狀態**
```bash
git status
git diff
```

2. **添加新檔案**
```bash
git add src/services/japanese_site_enhancer.py
git add src/services/web_searcher.py
git add test_encoding_fixes.py
```

3. **提交變更**
```bash
git commit -m "feat: 實施分離式編碼處理策略

- 新增日文網站專用編碼增強器
- 分離 av-wiki.net/chiba-f.net 與 JAVDB 的編碼處理
- 為日文網站設定較短請求間隔
- 保持 JAVDB 原有標準編碼處理
- 解決編碼警告問題"
```

### 🎯 預期結果

完成後應該達到：

1. **編碼問題解決**：
   - 日文網站無編碼警告
   - CP932 編碼正確解析日文內容

2. **JAVDB 功能正常**：
   - 搜尋成功率不變
   - 無編碼問題

3. **效能優化**：
   - 日文網站使用較短延遲（0.5-1.5秒）
   - JAVDB 維持原有延遲策略

4. **系統穩定性**：
   - 不同網站使用適合的編碼策略
   - 避免混合編碼問題

### ⚠️ 注意事項

1. **備份重要**：執行前確保有 Git 備份
2. **測試充分**：每個階段都要測試
3. **日誌監控**：注意觀察編碼警告是否消失
4. **效能驗證**：確認搜尋速度和成功率

### 🆘 回滾方案

如果出現問題：

```bash
# 回到修改前的狀態
git reset --hard HEAD~1

# 或回到指定提交
git reset --hard ab648ae
```

這個指南應該能讓 Claude Code 自動執行並完成分離式編碼處理的實施。
