# -*- coding: utf-8 -*-
"""
æ—¥æ–‡ç¶²ç«™å…§å®¹èª¿è©¦è…³æœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent / 'å¥³å„ªåˆ†é¡'
sys.path.insert(0, str(project_root / 'src'))

import httpx
from services.japanese_site_enhancer import create_japanese_soup
from urllib.parse import quote

def debug_av_wiki(code: str):
    """èª¿è©¦ av-wiki.net æœå°‹çµæœ"""
    search_url = f"https://av-wiki.net/?s={quote(code)}&post_type=product"
    
    print(f"ğŸ” èª¿è©¦ AV-WIKI æœå°‹: {code}")
    print(f"URL: {search_url}")
    print("-" * 80)
    
    try:
        with httpx.Client(timeout=20) as client:
            response = client.get(search_url)
            response.raise_for_status()
            
            # ä½¿ç”¨æ—¥æ–‡ç·¨ç¢¼å¢å¼·å™¨
            soup = create_japanese_soup(response, search_url)
            
            print("ğŸ“‹ é é¢åŸºæœ¬è³‡è¨Š:")
            print(f"  ç‹€æ…‹ç¢¼: {response.status_code}")
            print(f"  é é¢æ¨™é¡Œ: {soup.title.text if soup.title else 'None'}")
            print(f"  é é¢å¤§å°: {len(response.content)} bytes")
            
            print("\nğŸ­ å°‹æ‰¾å¥³å„ªåç¨±:")
            
            # æ–¹æ³•1: æŸ¥æ‰¾ class="actress-name"
            actress_elements = soup.find_all(class_="actress-name")
            print(f"  actress-name å…ƒç´ æ•¸é‡: {len(actress_elements)}")
            for i, elem in enumerate(actress_elements[:5]):
                print(f"    [{i}] {elem.text.strip()}")
            
            # æ–¹æ³•2: æŸ¥æ‰¾å¯èƒ½çš„å¥³å„ªç›¸é—œé¡åˆ¥
            potential_classes = [
                'actress', 'performer', 'model', 'star', 'talent',
                'å¥³å„ª', 'å‡ºæ¼”è€…', 'ã‚­ãƒ£ã‚¹ãƒˆ', 'ã‚¢ã‚¯ãƒˆãƒ¬ã‚¹'
            ]
            
            for class_name in potential_classes:
                elements = soup.find_all(class_=lambda x: x and class_name in x.lower() if x else False)
                if elements:
                    print(f"  æ‰¾åˆ° '{class_name}' ç›¸é—œå…ƒç´ : {len(elements)}")
                    for elem in elements[:3]:
                        print(f"    - {elem.text.strip()[:50]}")
            
            # æ–¹æ³•3: æŸ¥æ‰¾åŒ…å«ç•ªè™Ÿçš„å…§å®¹
            print(f"\nğŸ“„ åŒ…å«ç•ªè™Ÿ '{code}' çš„å…§å®¹:")
            page_text = soup.get_text()
            lines = page_text.split('\n')
            matching_lines = [line.strip() for line in lines if code in line.upper() and line.strip()]
            
            for i, line in enumerate(matching_lines[:10]):
                print(f"  [{i}] {line[:100]}")
            
            # æ–¹æ³•4: åˆ†æç¶²é çµæ§‹
            print(f"\nğŸ—ï¸ ç¶²é çµæ§‹åˆ†æ:")
            
            # æŸ¥æ‰¾ä¸»è¦å…§å®¹å€åŸŸ
            main_content = soup.find(['main', 'div'], class_=lambda x: x and ('content' in x.lower() or 'main' in x.lower()) if x else False)
            if main_content:
                print(f"  æ‰¾åˆ°ä¸»è¦å…§å®¹å€åŸŸ: {main_content.name} class={main_content.get('class')}")
            
            # æŸ¥æ‰¾ç”¢å“/å½±ç‰‡ç›¸é—œå€åŸŸ
            product_areas = soup.find_all(['div', 'article'], class_=lambda x: x and ('product' in x.lower() or 'post' in x.lower() or 'item' in x.lower()) if x else False)
            print(f"  æ‰¾åˆ°ç”¢å“ç›¸é—œå€åŸŸ: {len(product_areas)}")
            
            for i, area in enumerate(product_areas[:3]):
                print(f"    ç”¢å“å€åŸŸ [{i}]: {area.name} class={area.get('class')}")
                area_text = area.get_text()[:200].replace('\n', ' ')
                print(f"      å…§å®¹: {area_text}")
            
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")

def debug_chiba_f_net(code: str):
    """èª¿è©¦ chiba-f.net æœå°‹çµæœ"""
    search_url = f"https://chiba-f.net/search/?keyword={quote(code)}"
    
    print(f"ğŸ” èª¿è©¦ chiba-f.net æœå°‹: {code}")
    print(f"URL: {search_url}")
    print("-" * 80)
    
    try:
        with httpx.Client(timeout=20) as client:
            response = client.get(search_url)
            response.raise_for_status()
            
            # ä½¿ç”¨æ—¥æ–‡ç·¨ç¢¼å¢å¼·å™¨
            soup = create_japanese_soup(response, search_url)
            
            print("ğŸ“‹ é é¢åŸºæœ¬è³‡è¨Š:")
            print(f"  ç‹€æ…‹ç¢¼: {response.status_code}")
            print(f"  é é¢æ¨™é¡Œ: {soup.title.text if soup.title else 'None'}")
            print(f"  é é¢å¤§å°: {len(response.content)} bytes")
            
            print("\nğŸ­ å°‹æ‰¾å¥³å„ªåç¨±:")
            
            # æŸ¥æ‰¾ product-div
            product_divs = soup.find_all('div', class_='product-div')
            print(f"  product-div æ•¸é‡: {len(product_divs)}")
            
            for i, div in enumerate(product_divs[:3]):
                print(f"\n  ç”¢å“ [{i}]:")
                
                # æŸ¥æ‰¾ç•ªè™Ÿ
                pno_element = div.find('div', class_='pno')
                if pno_element:
                    print(f"    ç•ªè™Ÿ: {pno_element.text.strip()}")
                
                # æŸ¥æ‰¾å¥³å„ªåç¨±
                actress_span = div.find('span', class_='fw-bold')
                if actress_span:
                    print(f"    å¥³å„ª: {actress_span.text.strip()}")
                
                # æŸ¥æ‰¾ç³»åˆ—
                series_link = div.find('a', href=lambda x: x and '../series/' in x if x else False)
                if series_link:
                    print(f"    ç³»åˆ—: {series_link.text.strip()}")
                
                # é¡¯ç¤ºå®Œæ•´å…§å®¹
                div_text = div.get_text()[:300].replace('\n', ' ')
                print(f"    å…§å®¹: {div_text}")
            
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    # æ¸¬è©¦å¹¾å€‹ç•ªè™Ÿ
    test_codes = ['SONE-323', 'SIVR-345']
    
    for code in test_codes:
        print("=" * 100)
        debug_av_wiki(code)
        print("\n")
        debug_chiba_f_net(code)
        print("=" * 100)
        print()
