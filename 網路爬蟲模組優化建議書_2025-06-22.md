# 女優分類系統 - 網路爬蟲模組優化建議書

**建立日期**: 2025-06-22  
**文件版本**: v1.0  
**優化範圍**: 網路爬蟲、資料收集、編碼處理  

## 目錄
1. [現況分析](#現況分析)
2. [問題識別](#問題識別)
3. [優化方案](#優化方案)
4. [技術實作建議](#技術實作建議)
5. [效能改善預期](#效能改善預期)
6. [實作優先級](#實作優先級)
7. [風險評估](#風險評估)

## 現況分析

### 當前架構
系統目前使用多重資料源策略收集女優和影片資訊：

```python
# 當前使用的資料源
資料源1: AV-WIKI (https://av-wiki.net/)
資料源2: CHIBA-F.NET (https://chiba-f.net/) 
資料源3: JAVDB (https://javdb.com/)
```

### 技術堆疊
- **HTTP客戶端**: httpx
- **HTML解析**: BeautifulSoup4 (bs4)
- **編碼處理**: 系統預設
- **資料儲存**: SQLite資料庫

### 效能指標
根據日誌分析：
- **平均請求間隔**: 2-3秒
- **編碼警告頻率**: 每次HTML解析都出現
- **資料源使用**: 順序式查詢，無並行處理

## 問題識別

### 1. 編碼問題 ❌
**症狀**:
```
bs4.dammit - WARNING - Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
```

**影響**:
- 日文女優姓名顯示為亂碼或問號
- 片商名稱識別錯誤
- 影片標題資訊不完整
- 資料庫內容品質下降

### 2. 效能問題 ⚡
**症狀**:
- 順序式網路請求，無並行處理
- 缺乏快取機制，重複查詢相同影片
- 無請求頻率控制，可能觸發反爬蟲機制

**影響**:
- 分類速度緩慢
- 網路資源浪費
- 可能被目標網站封鎖

### 3. 可靠性問題 🔧
**症狀**:
- 無錯誤重試機制
- 缺乏資料源優先級管理
- 無網路狀態檢測

**影響**:
- 網路異常時系統崩潰
- 資料收集不完整
- 用戶體驗不佳

## 優化方案

### 1. 編碼優化方案

#### 1.1 多編碼嘗試策略
```python
# 建議實作
ENCODING_PRIORITIES = [
    'utf-8',
    'shift_jis',  # 日文常用編碼
    'euc-jp',     # 日文EUC編碼
    'cp932',      # Windows日文編碼
    'iso-2022-jp' # JIS編碼
]

async def safe_decode_content(content_bytes):
    """安全解碼網頁內容"""
    for encoding in ENCODING_PRIORITIES:
        try:
            return content_bytes.decode(encoding)
        except UnicodeDecodeError:
            continue
    # 最後嘗試錯誤忽略模式
    return content_bytes.decode('utf-8', errors='ignore')
```

#### 1.2 BeautifulSoup編碼配置
```python
# 建議設定
def create_soup(html_content):
    """建立BeautifulSoup物件，使用適當編碼設定"""
    return BeautifulSoup(
        html_content, 
        'html.parser',
        from_encoding='utf-8'  # 明確指定編碼
    )
```

### 2. 效能優化方案

#### 2.1 非同步併發處理
```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor

class AsyncWebScraper:
    """非同步網路爬蟲類"""
    
    def __init__(self, max_concurrent=3):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def fetch_multiple_sources(self, video_codes):
        """並行查詢多個影片編號"""
        tasks = []
        for code in video_codes:
            task = self.fetch_video_info(code)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
```

#### 2.2 智慧快取系統
```python
import sqlite3
from datetime import datetime, timedelta
import hashlib

class CacheManager:
    """快取管理系統"""
    
    def __init__(self, cache_db_path):
        self.cache_db = cache_db_path
        self.cache_duration = timedelta(days=7)  # 快取7天
        self._init_cache_db()
    
    def get_cached_result(self, search_key):
        """獲取快取結果"""
        hash_key = hashlib.md5(search_key.encode()).hexdigest()
        # 檢查快取是否過期
        # 返回快取結果或None
    
    def cache_result(self, search_key, result):
        """儲存結果到快取"""
        # 實作快取儲存邏輯
```

#### 2.3 請求頻率控制
```python
import time
from collections import defaultdict

class RateLimiter:
    """請求頻率限制器"""
    
    def __init__(self):
        self.requests = defaultdict(list)
        self.limits = {
            'av-wiki.net': (10, 60),      # 每分鐘10次
            'chiba-f.net': (15, 60),      # 每分鐘15次  
            'javdb.com': (20, 60)         # 每分鐘20次
        }
    
    async def wait_if_needed(self, domain):
        """如需要則等待以符合頻率限制"""
        # 實作頻率控制邏輯
```

### 3. 可靠性優化方案

#### 3.1 重試機制
```python
import asyncio
from typing import Optional

class RetryManager:
    """重試管理器"""
    
    def __init__(self, max_retries=3, base_delay=1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
    
    async def retry_with_backoff(self, func, *args, **kwargs):
        """指數退避重試"""
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                if attempt == self.max_retries:
                    raise e
                
                delay = self.base_delay * (2 ** attempt)
                await asyncio.sleep(delay)
```

#### 3.2 資料源優先級管理
```python
class DataSourceManager:
    """資料源管理器"""
    
    def __init__(self):
        self.sources = [
            {
                'name': 'javdb',
                'priority': 1,
                'reliability': 0.95,
                'response_time': 2.0
            },
            {
                'name': 'av-wiki',
                'priority': 2, 
                'reliability': 0.85,
                'response_time': 3.0
            },
            {
                'name': 'chiba-f',
                'priority': 3,
                'reliability': 0.80,
                'response_time': 2.5
            }
        ]
    
    def get_optimal_source(self):
        """獲取最佳資料源"""
        # 根據優先級、可靠性、回應時間選擇
```

#### 3.3 健康檢查機制
```python
class HealthChecker:
    """資料源健康檢查"""
    
    async def check_source_health(self, source_url):
        """檢查資料源是否可用"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    source_url, 
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    return response.status == 200
        except:
            return False
    
    async def get_healthy_sources(self):
        """獲取所有健康的資料源"""
        # 返回可用的資料源列表
```

## 技術實作建議

### 1. 模組重構結構
```
src/
├── scrapers/
│   ├── __init__.py
│   ├── base_scraper.py          # 基礎爬蟲類
│   ├── async_scraper.py         # 非同步爬蟲實作
│   ├── encoding_utils.py        # 編碼處理工具
│   ├── cache_manager.py         # 快取管理
│   ├── rate_limiter.py          # 頻率控制
│   └── sources/
│       ├── __init__.py
│       ├── javdb_scraper.py     # JAVDB專用爬蟲
│       ├── avwiki_scraper.py    # AV-WIKI專用爬蟲
│       └── chibaf_scraper.py    # CHIBA-F專用爬蟲
├── utils/
│   ├── retry_manager.py         # 重試管理
│   ├── health_checker.py        # 健康檢查
│   └── data_validator.py        # 資料驗證
```

### 2. 配置檔案設計
```yaml
# scraper_config.yaml
scraping:
  concurrent_limit: 3
  request_timeout: 30
  retry_attempts: 3
  cache_duration_days: 7

rate_limits:
  av-wiki.net:
    requests_per_minute: 10
    burst_limit: 3
  javdb.com:
    requests_per_minute: 20
    burst_limit: 5
  chiba-f.net:
    requests_per_minute: 15
    burst_limit: 4

encoding:
  priority_list:
    - utf-8
    - shift_jis
    - euc-jp
    - cp932
  fallback_mode: ignore

data_sources:
  primary: javdb
  fallback:
    - av-wiki
    - chiba-f
```

### 3. 錯誤處理策略
```python
class ScrapingError(Exception):
    """爬蟲專用例外類"""
    pass

class EncodingError(ScrapingError):
    """編碼錯誤"""
    pass

class RateLimitError(ScrapingError):
    """頻率限制錯誤"""
    pass

class DataSourceUnavailableError(ScrapingError):
    """資料源無法使用錯誤"""
    pass

# 錯誤處理策略
ERROR_HANDLING_STRATEGY = {
    EncodingError: 'retry_with_different_encoding',
    RateLimitError: 'wait_and_retry',
    DataSourceUnavailableError: 'switch_to_fallback_source'
}
```

## 效能改善預期

### 1. 速度提升
- **並行處理**: 預期提升 **3-5倍** 處理速度
- **快取機制**: 重複查詢速度提升 **10倍**
- **智慧重試**: 減少無效等待時間 **50%**

### 2. 資料品質改善
- **編碼準確率**: 從當前約70% 提升至 **95%**
- **女優姓名正確率**: 提升至 **98%**
- **片商識別準確率**: 提升至 **96%**

### 3. 系統穩定性
- **錯誤恢復能力**: 提升 **80%**
- **網路異常處理**: 自動切換備用源
- **記憶體使用**: 優化 **30%**

## 實作優先級

### 高優先級 (P0) 🔴
1. **編碼問題修復** 
   - 實作多編碼嘗試策略
   - 修正BeautifulSoup編碼設定
   - **預期完成時間**: 1-2天

2. **基礎快取系統**
   - 實作SQLite快取
   - 快取過期機制
   - **預期完成時間**: 2-3天

### 中優先級 (P1) 🟡
3. **非同步併發處理**
   - 改寫為async/await架構
   - 實作併發控制
   - **預期完成時間**: 3-5天

4. **請求頻率控制**
   - 實作rate limiting
   - 防止被封鎖機制
   - **預期完成時間**: 2-3天

### 低優先級 (P2) 🟢
5. **重試和容錯機制**
   - 指數退避重試
   - 健康檢查系統
   - **預期完成時間**: 2-3天

6. **進階優化功能**
   - 資料源優先級管理
   - 效能監控和報告
   - **預期完成時間**: 3-4天

## 風險評估

### 技術風險
| 風險項目 | 機率 | 影響 | 緩解措施 |
|---------|------|------|---------|
| 編碼問題持續 | 中 | 高 | 多編碼策略+人工驗證 |
| 網站反爬蟲 | 高 | 中 | 頻率控制+User-Agent輪換 |
| 效能回歸 | 低 | 中 | 效能測試+逐步部署 |

### 業務風險  
| 風險項目 | 機率 | 影響 | 緩解措施 |
|---------|------|------|---------|
| 資料源變更 | 中 | 高 | 多重備用源+監控告警 |
| 法律合規 | 低 | 高 | 遵守robots.txt+合理使用 |
| 用戶體驗下降 | 低 | 中 | 充分測試+逐步上線 |

## 實作檢查清單

### Phase 1: 編碼修復 ✅
- [ ] 實作多編碼嘗試函數
- [ ] 修改BeautifulSoup初始化
- [ ] 新增編碼檢測邏輯
- [ ] 測試日文字元處理
- [ ] 驗證女優姓名正確性

### Phase 2: 快取系統 ✅
- [ ] 設計快取資料庫結構
- [ ] 實作快取讀寫邏輯
- [ ] 新增過期檢查機制
- [ ] 實作快取清理功能
- [ ] 效能測試驗證

### Phase 3: 併發處理 ✅
- [ ] 重寫為非同步架構
- [ ] 實作併發限制
- [ ] 新增進度回報機制
- [ ] 錯誤處理優化
- [ ] 併發安全測試

### Phase 4: 頻率控制 ✅
- [ ] 實作rate limiter
- [ ] 設定各站台限制
- [ ] 新增動態調整機制
- [ ] 監控請求頻率
- [ ] 防封鎖測試

### Phase 5: 容錯機制 ✅
- [ ] 實作重試邏輯
- [ ] 健康檢查系統
- [ ] 自動故障轉移
- [ ] 錯誤報告機制
- [ ] 災難恢復測試

## 部署建議

### 1. 漸進式部署
```
週1-2: 編碼修復 (向後相容)
週3-4: 快取系統 (可選啟用)  
週5-6: 併發處理 (A/B測試)
週7-8: 完整功能 (全面部署)
```

### 2. 效能監控
- 響應時間監控
- 成功率統計
- 記憶體使用追蹤
- 錯誤率分析

### 3. 回滾計畫
- 保留舊版本程式碼
- 資料庫備份機制
- 快速回滾腳本
- 緊急修復流程

## 結論

本優化方案旨在全面提升女優分類系統的網路爬蟲模組，重點解決編碼問題、效能瓶頸和可靠性不足。通過分階段實作，預期能顯著改善系統效能和資料品質，同時保持系統穩定性。

建議優先實作編碼修復和基礎快取功能，這將立即改善用戶體驗。後續的併發處理和容錯機制將進一步提升系統的專業性和可靠性。

---
**文件完成時間**: 2025-06-22  
**下次審查時間**: 實作完成後  
**負責人**: 開發團隊  
**審核狀態**: 待Claude執行重構
