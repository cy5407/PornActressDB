#!/bin/bash
# ðŸš€ WSL GPU ä¸€éµè¨­å®šè…³æœ¬
# è‡ªå‹•é…ç½® GPU åŠ é€Ÿç’°å¢ƒ

set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³åœæ­¢

echo "ðŸš€ WSL GPU åŠ é€Ÿç’°å¢ƒè‡ªå‹•è¨­å®šé–‹å§‹"
echo "=========================================="

# é¡è‰²å®šç¾©
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# é€²åº¦é¡¯ç¤ºå‡½æ•¸
show_progress() {
    echo -e "${BLUE}[é€²è¡Œä¸­]${NC} $1"
}

show_success() {
    echo -e "${GREEN}[å®Œæˆ]${NC} $1"
}

show_warning() {
    echo -e "${YELLOW}[è­¦å‘Š]${NC} $1"
}

show_error() {
    echo -e "${RED}[éŒ¯èª¤]${NC} $1"
}

# æª¢æŸ¥ç³»çµ±éœ€æ±‚
check_requirements() {
    show_progress "æª¢æŸ¥ç³»çµ±éœ€æ±‚..."
    
    # æª¢æŸ¥ WSL ç‰ˆæœ¬
    if command -v wsl.exe &> /dev/null; then
        WSL_VERSION=$(wsl.exe --version 2>/dev/null | head -1 || echo "WSL 1")
        echo "   WSL ç‰ˆæœ¬: $WSL_VERSION"
    else
        show_warning "ç„¡æ³•æª¢æ¸¬ WSL ç‰ˆæœ¬"
    fi
    
    # æª¢æŸ¥ Python ç‰ˆæœ¬
    if command -v python3 &> /dev/null; then
        PYTHON_VERSION=$(python3 --version)
        echo "   Python ç‰ˆæœ¬: $PYTHON_VERSION"
    else
        show_error "Python3 æœªå®‰è£"
        exit 1
    fi
    
    # æª¢æŸ¥ pip
    if command -v pip3 &> /dev/null; then
        show_success "pip3 å·²å®‰è£"
    else
        show_error "pip3 æœªå®‰è£"
        exit 1
    fi
    
    show_success "ç³»çµ±éœ€æ±‚æª¢æŸ¥å®Œæˆ"
}

# æ›´æ–°ç³»çµ±å¥—ä»¶
update_system() {
    show_progress "æ›´æ–°ç³»çµ±å¥—ä»¶..."
    
    sudo apt update -qq
    sudo apt upgrade -y -qq
    
    # å®‰è£å¿…è¦å·¥å…·
    sudo apt install -y -qq \
        python3-pip \
        python3-venv \
        python3-dev \
        build-essential \
        cmake \
        git \
        curl \
        wget \
        htop \
        tree
    
    show_success "ç³»çµ±å¥—ä»¶æ›´æ–°å®Œæˆ"
}

# æª¢æŸ¥ GPU å¯ç”¨æ€§
check_gpu() {
    show_progress "æª¢æŸ¥ GPU å¯ç”¨æ€§..."
    
    # æª¢æŸ¥ NVIDIA GPU
    if command -v nvidia-smi &> /dev/null; then
        echo "   NVIDIA GPU è³‡è¨Š:"
        nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader | head -1
        GPU_TYPE="nvidia"
        show_success "åµæ¸¬åˆ° NVIDIA GPU"
    else
        show_warning "æœªåµæ¸¬åˆ° NVIDIA GPU æˆ–é©…å‹•ç¨‹å¼"
        GPU_TYPE="cpu"
    fi
    
    # æª¢æŸ¥å…¶ä»– GPU (AMD, Intel)
    if lspci | grep -i amd &> /dev/null; then
        echo "   åµæ¸¬åˆ° AMD GPU"
        GPU_TYPE="amd"
    fi
    
    if lspci | grep -i intel.*graphics &> /dev/null; then
        echo "   åµæ¸¬åˆ° Intel GPU"
        if [ "$GPU_TYPE" == "cpu" ]; then
            GPU_TYPE="intel"
        fi
    fi
}

# å»ºç«‹ GPU è™›æ“¬ç’°å¢ƒ
create_gpu_environment() {
    show_progress "å»ºç«‹ GPU è™›æ“¬ç’°å¢ƒ..."
    
    # ç§»åˆ°å°ˆæ¡ˆç›®éŒ„
    PROJECT_DIR="/mnt/c/Users/$USER/OneDrive/æ¡Œé¢/Python/å¥³å„ªåˆ†é¡ž_é‡æ§‹20250617"
    if [ -d "$PROJECT_DIR" ]; then
        cd "$PROJECT_DIR"
    else
        show_warning "å°ˆæ¡ˆç›®éŒ„ä¸å­˜åœ¨ï¼Œåœ¨ç•¶å‰ç›®éŒ„å»ºç«‹ç’°å¢ƒ"
    fi
    
    # å»ºç«‹è™›æ“¬ç’°å¢ƒ
    if [ -d "gpu_env" ]; then
        show_warning "GPU ç’°å¢ƒå·²å­˜åœ¨ï¼Œå°‡é‡æ–°å»ºç«‹"
        rm -rf gpu_env
    fi
    
    python3 -m venv gpu_env
    source gpu_env/bin/activate
    
    # å‡ç´š pip
    pip install --upgrade pip -q
    
    show_success "GPU è™›æ“¬ç’°å¢ƒå»ºç«‹å®Œæˆ"
}

# å®‰è£ GPU å¥—ä»¶
install_gpu_packages() {
    show_progress "å®‰è£ GPU åŠ é€Ÿå¥—ä»¶..."
    
    # ç¢ºä¿åœ¨è™›æ“¬ç’°å¢ƒä¸­
    source gpu_env/bin/activate
    
    # åŸºç¤Žå¥—ä»¶
    pip install -q numpy scipy matplotlib pandas scikit-learn
    
    # æ ¹æ“š GPU é¡žåž‹å®‰è£å°æ‡‰å¥—ä»¶
    case $GPU_TYPE in
        "nvidia")
            show_progress "å®‰è£ NVIDIA CUDA å¥—ä»¶..."
            pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
            pip install -q tensorflow[and-cuda]
            pip install -q cupy-cuda12x
            show_success "NVIDIA å¥—ä»¶å®‰è£å®Œæˆ"
            ;;
        "amd")
            show_progress "å®‰è£ AMD ROCm å¥—ä»¶..."
            pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7
            pip install -q tensorflow-rocm
            show_success "AMD å¥—ä»¶å®‰è£å®Œæˆ"
            ;;
        "intel")
            show_progress "å®‰è£ Intel GPU å¥—ä»¶..."
            pip install -q intel-extension-for-pytorch
            pip install -q intel-tensorflow
            show_success "Intel å¥—ä»¶å®‰è£å®Œæˆ"
            ;;
        *)
            show_warning "æœªåµæ¸¬åˆ° GPUï¼Œå®‰è£ CPU ç‰ˆæœ¬å¥—ä»¶"
            pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
            pip install -q tensorflow
            ;;
    esac
    
    # å®‰è£å…¶ä»–å¿…è¦å¥—ä»¶
    pip install -q opencv-python Pillow requests beautifulsoup4 lxml
    
    show_success "GPU å¥—ä»¶å®‰è£å®Œæˆ"
}

# å»ºç«‹æ¸¬è©¦è…³æœ¬
create_test_scripts() {
    show_progress "å»ºç«‹ GPU æ¸¬è©¦è…³æœ¬..."
    
    # å»ºç«‹å¿«é€Ÿæ¸¬è©¦è…³æœ¬
    cat > quick_gpu_test.py << 'EOF'
#!/usr/bin/env python3
"""å¿«é€Ÿ GPU æ¸¬è©¦è…³æœ¬"""
import sys

def test_pytorch():
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
            print(f"âœ… GPU æ•¸é‡: {torch.cuda.device_count()}")
            print(f"âœ… GPU åç¨±: {torch.cuda.get_device_name(0)}")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨")
        return True
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£")
        return False

def test_tensorflow():
    try:
        import tensorflow as tf
        print(f"âœ… TensorFlow ç‰ˆæœ¬: {tf.__version__}")
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"âœ… TensorFlow GPU æ•¸é‡: {len(gpus)}")
        else:
            print("âš ï¸  TensorFlow GPU ä¸å¯ç”¨")
        return True
    except ImportError:
        print("âŒ TensorFlow æœªå®‰è£")
        return False

def quick_performance_test():
    try:
        import torch
        import time
        
        if not torch.cuda.is_available():
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³éŽæ•ˆèƒ½æ¸¬è©¦")
            return
        
        print("ðŸš€ åŸ·è¡Œå¿«é€Ÿæ•ˆèƒ½æ¸¬è©¦...")
        
        # å»ºç«‹æ¸¬è©¦å¼µé‡
        size = 2048
        a = torch.randn(size, size, device='cpu')
        b = torch.randn(size, size, device='cpu')
        
        # CPU æ¸¬è©¦
        start = time.time()
        c_cpu = torch.matmul(a, b)
        cpu_time = time.time() - start
        
        # GPU æ¸¬è©¦
        a_gpu = a.cuda()
        b_gpu = b.cuda()
        torch.cuda.synchronize()
        
        start = time.time()
        c_gpu = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        
        speedup = cpu_time / gpu_time
        print(f"ðŸŽ¯ CPU æ™‚é–“: {cpu_time:.4f}s")
        print(f"ðŸŽ¯ GPU æ™‚é–“: {gpu_time:.4f}s")
        print(f"ðŸŽ¯ åŠ é€Ÿå€æ•¸: {speedup:.2f}x")
        
    except Exception as e:
        print(f"âŒ æ•ˆèƒ½æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    print("ðŸ” GPU ç’°å¢ƒå¿«é€Ÿæ¸¬è©¦")
    print("=" * 40)
    
    pytorch_ok = test_pytorch()
    tensorflow_ok = test_tensorflow()
    
    if pytorch_ok:
        quick_performance_test()
    
    print("\nâœ… æ¸¬è©¦å®Œæˆï¼")
EOF
    
    chmod +x quick_gpu_test.py
    show_success "æ¸¬è©¦è…³æœ¬å»ºç«‹å®Œæˆ"
}

# å»ºç«‹å•Ÿå‹•è…³æœ¬
create_launch_scripts() {
    show_progress "å»ºç«‹å•Ÿå‹•è…³æœ¬..."
    
    # GPU ç’°å¢ƒå•Ÿå‹•è…³æœ¬
    cat > start_gpu_env.sh << 'EOF'
#!/bin/bash
# GPU ç’°å¢ƒå•Ÿå‹•è…³æœ¬

echo "ðŸ”¥ å•Ÿå‹• GPU åŠ é€Ÿç’°å¢ƒ"
echo "====================="

# å•Ÿç”¨è™›æ“¬ç’°å¢ƒ
if [ -f "gpu_env/bin/activate" ]; then
    source gpu_env/bin/activate
    echo "âœ… GPU è™›æ“¬ç’°å¢ƒå·²å•Ÿç”¨"
else
    echo "âŒ GPU ç’°å¢ƒä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œè¨­å®šè…³æœ¬"
    exit 1
fi

# é¡¯ç¤º GPU è³‡è¨Š
python quick_gpu_test.py

echo ""
echo "ðŸŽ¯ ç’°å¢ƒå·²å°±ç·’ï¼å¯ä»¥é–‹å§‹ä½¿ç”¨ GPU åŠ é€ŸåŠŸèƒ½"
echo "åŸ·è¡Œ 'python your_script.py' é–‹å§‹é–‹ç™¼"
EOF
    
    chmod +x start_gpu_env.sh
    
    # å°ˆæ¡ˆå•Ÿå‹•è…³æœ¬ (å¦‚æžœå­˜åœ¨)
    if [ -f "run.py" ]; then
        cat > run_gpu.sh << 'EOF'
#!/bin/bash
# å¥³å„ªåˆ†é¡žç³»çµ± GPU ç‰ˆæœ¬å•Ÿå‹•è…³æœ¬

echo "ðŸŽ¬ å¥³å„ªåˆ†é¡žç³»çµ± - GPU åŠ é€Ÿç‰ˆ"
echo "============================="

# å•Ÿç”¨ GPU ç’°å¢ƒ
source gpu_env/bin/activate

# åŸ·è¡Œç³»çµ±
python run.py --use-gpu "$@"
EOF
        chmod +x run_gpu.sh
        show_success "å°ˆæ¡ˆ GPU å•Ÿå‹•è…³æœ¬å»ºç«‹å®Œæˆ"
    fi
    
    show_success "å•Ÿå‹•è…³æœ¬å»ºç«‹å®Œæˆ"
}

# åŸ·è¡Œæœ€çµ‚æ¸¬è©¦
run_final_test() {
    show_progress "åŸ·è¡Œæœ€çµ‚ GPU æ¸¬è©¦..."
    
    source gpu_env/bin/activate
    python quick_gpu_test.py
    
    show_success "GPU ç’°å¢ƒè¨­å®šå®Œæˆï¼"
}

# é¡¯ç¤ºä½¿ç”¨èªªæ˜Ž
show_usage_instructions() {
    echo ""
    echo "ðŸŽ‰ GPU åŠ é€Ÿç’°å¢ƒè¨­å®šå®Œæˆï¼"
    echo "=========================================="
    echo ""
    echo "ðŸ“‹ ä½¿ç”¨èªªæ˜Ž:"
    echo "   1. å•Ÿå‹• GPU ç’°å¢ƒ: ./start_gpu_env.sh"
    echo "   2. åŸ·è¡Œæ¸¬è©¦: python quick_gpu_test.py"
    if [ -f "run_gpu.sh" ]; then
        echo "   3. å•Ÿå‹•å°ˆæ¡ˆ: ./run_gpu.sh"
    fi
    echo ""
    echo "ðŸ”§ æ‰‹å‹•å•Ÿå‹•:"
    echo "   source gpu_env/bin/activate"
    echo "   python your_script.py"
    echo ""
    echo "ðŸ“Š æ•ˆèƒ½é æœŸ:"
    echo "   - çŸ©é™£é‹ç®—: 10-50x åŠ é€Ÿ"
    echo "   - åœ–åƒè™•ç†: 5-20x åŠ é€Ÿ"
    echo "   - æ·±åº¦å­¸ç¿’: 20-100x åŠ é€Ÿ"
    echo ""
    echo "ðŸš€ é–‹å§‹ä½¿ç”¨ GPU åŠ é€Ÿé€²è¡Œé–‹ç™¼å§ï¼"
}

# ä¸»ç¨‹å¼åŸ·è¡Œæµç¨‹
main() {
    echo "é–‹å§‹è‡ªå‹•è¨­å®š GPU åŠ é€Ÿç’°å¢ƒ..."
    echo ""
    
    check_requirements
    echo ""
    
    update_system
    echo ""
    
    check_gpu
    echo ""
    
    create_gpu_environment
    echo ""
    
    install_gpu_packages
    echo ""
    
    create_test_scripts
    echo ""
    
    create_launch_scripts
    echo ""
    
    run_final_test
    echo ""
    
    show_usage_instructions
}

# éŒ¯èª¤è™•ç†
trap 'show_error "è¨­å®šéŽç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼"; exit 1' ERR

# åŸ·è¡Œä¸»ç¨‹å¼
main

exit 0
