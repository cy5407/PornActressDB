#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ httpx è‡ªå‹•è§£å£“å’Œæ‰‹å‹•è™•ç†
"""

import httpx
import brotli
from bs4 import BeautifulSoup

def test_httpx_decompression():
    """æ¸¬è©¦ httpx çš„è‡ªå‹•è§£å£“åŠŸèƒ½"""
    print("ğŸš€ æ¸¬è©¦ httpx è‡ªå‹•è§£å£“ vs æ‰‹å‹•è™•ç†")
    print("=" * 80)
    
    url = "https://av-wiki.net/?s=SONE-553&post_type=product"
    
    # æ¸¬è©¦ 1: è®“ httpx è‡ªå‹•è™•ç†å£“ç¸®
    print("\nğŸ“‹ æ¸¬è©¦ 1: httpx è‡ªå‹•è™•ç†")
    print("-" * 40)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
    }
    
    try:
        with httpx.Client(timeout=30) as client:
            response = client.get(url, headers=headers)
            
            print(f"ğŸ“Š ç‹€æ…‹ç¢¼: {response.status_code}")
            print(f"ğŸ“ response.content é•·åº¦: {len(response.content)}")
            print(f"ğŸ“„ response.text é•·åº¦: {len(response.text)}")
            print(f"ğŸ—œï¸ content-encoding: {response.headers.get('content-encoding', 'none')}")
            print(f"ğŸ”¤ charset: {response.charset}")
            
            # æª¢æŸ¥ response.text æ˜¯å¦å¯è®€
            text = response.text
            if '<html' in text.lower() and '</html>' in text.lower():
                print("âœ… response.text åŒ…å«æœ‰æ•ˆ HTML")
                
                soup = BeautifulSoup(text, 'html.parser')
                print(f"ğŸ“„ è§£æåˆ° {len(soup.find_all())} å€‹ HTML æ¨™ç±¤")
                
                # æª¢æŸ¥é é¢æ¨™é¡Œ
                title = soup.find('title')
                if title:
                    print(f"ğŸ“‹ é é¢æ¨™é¡Œ: {title.get_text().strip()}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æœå°‹çµæœ
                actress_elements = soup.find_all(class_="actress-name")
                print(f"ğŸ­ actress-name å…ƒç´ : {len(actress_elements)}")
                
                # æª¢æŸ¥å…¶ä»–å¯èƒ½çš„å¥³å„ªç›¸é—œå…ƒç´ 
                potential_selectors = [
                    'a[href*="actress"]',
                    '.performer', '.cast', '.star',
                    '[class*="actress"]', '[class*="performer"]',
                    'a[href*="star"]'
                ]
                
                for selector in potential_selectors:
                    elements = soup.select(selector)
                    if elements:
                        print(f"ğŸ” {selector}: {len(elements)} å€‹å…ƒç´ ")
                        for i, elem in enumerate(elements[:2]):  # é¡¯ç¤ºå‰2å€‹
                            text_content = elem.get_text().strip()
                            href = elem.get('href', '')
                            print(f"    {i+1}. {text_content[:30]}... (href: {href[:50]}...)")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ "No results" æˆ–é¡ä¼¼çš„æ–‡å­—
                page_text = soup.get_text().lower()
                no_result_indicators = ['no results', 'no matches', 'not found', 'æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“', 'è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“']
                for indicator in no_result_indicators:
                    if indicator in page_text:
                        print(f"âš ï¸ ç™¼ç¾ 'ç„¡çµæœ' æŒ‡ç¤ºå™¨: {indicator}")
                        
                # é¡¯ç¤ºé é¢ä¸»è¦çµæ§‹
                main_containers = soup.find_all(['main', 'div'], class_=lambda x: x and any(
                    keyword in x.lower() for keyword in ['content', 'main', 'search', 'result']
                ))
                print(f"ğŸ“¦ ä¸»è¦å®¹å™¨: {len(main_containers)} å€‹")
                
                if main_containers:
                    main_text = main_containers[0].get_text().strip()[:300]
                    print(f"ğŸ“– ä¸»è¦å…§å®¹é è¦½: {main_text}...")
                    
            else:
                print("âŒ response.text ä¸åŒ…å«æœ‰æ•ˆ HTML")
                # æª¢æŸ¥å‰ 200 å­—ç¬¦
                preview = text[:200]
                print(f"ğŸ“„ æ–‡å­—å…§å®¹é è¦½: {preview}...")
                
                # æª¢æŸ¥æ˜¯å¦ä»ç„¶æ˜¯äºŒé€²åˆ¶æ•¸æ“š
                if any(ord(c) > 127 for c in preview):
                    print("âš ï¸ å…§å®¹åŒ…å«é ASCII å­—ç¬¦ï¼Œå¯èƒ½ä»æ˜¯ç·¨ç¢¼å•é¡Œ")
                    
                    # å˜—è©¦ä¸åŒç·¨ç¢¼
                    encodings = ['cp932', 'shift_jis', 'euc-jp', 'iso-2022-jp']
                    for encoding in encodings:
                        try:
                            decoded = response.content.decode(encoding, errors='replace')
                            replacement_ratio = decoded.count('\ufffd') / len(decoded) if decoded else 1.0
                            print(f"  {encoding}: æ›¿æ›æ¯”ä¾‹ {replacement_ratio:.3f}")
                            if replacement_ratio < 0.1:
                                print(f"    âœ… {encoding} å¯èƒ½æœ‰æ•ˆ")
                                break
                        except:
                            pass
                            
    except Exception as e:
        print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")

    # æ¸¬è©¦ 2: ç¦ç”¨è‡ªå‹•è§£å£“ç¸®
    print(f"\nğŸ“‹ æ¸¬è©¦ 2: ç¦ç”¨è‡ªå‹•è§£å£“ç¸®")
    print("-" * 40)
    
    headers_no_encoding = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        # ä¸åŒ…å« Accept-Encoding ä»¥é¿å…å£“ç¸®
    }
    
    try:
        with httpx.Client(timeout=30) as client:
            response = client.get(url, headers=headers_no_encoding)
            
            print(f"ğŸ“Š ç‹€æ…‹ç¢¼: {response.status_code}")
            print(f"ğŸ“ content é•·åº¦: {len(response.content)}")
            print(f"ğŸ—œï¸ content-encoding: {response.headers.get('content-encoding', 'none')}")
            
            text = response.text
            if '<html' in text.lower() and '</html>' in text.lower():
                print("âœ… æœªå£“ç¸®ç‰ˆæœ¬åŒ…å«æœ‰æ•ˆ HTML")
                
                soup = BeautifulSoup(text, 'html.parser')
                print(f"ğŸ“„ è§£æåˆ° {len(soup.find_all())} å€‹ HTML æ¨™ç±¤")
                
                actress_elements = soup.find_all(class_="actress-name")
                print(f"ğŸ­ actress-name å…ƒç´ : {len(actress_elements)}")
                
            else:
                print("âŒ æœªå£“ç¸®ç‰ˆæœ¬ä¹Ÿä¸åŒ…å«æœ‰æ•ˆ HTML")
                
    except Exception as e:
        print(f"âŒ ç¦ç”¨å£“ç¸®çš„è«‹æ±‚å¤±æ•—: {e}")

if __name__ == "__main__":
    test_httpx_decompression()
