#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HTTP å›æ‡‰åˆ†æè…³æœ¬ - æª¢æŸ¥å£“ç¸®å’Œå…§å®¹å•é¡Œ
"""

import httpx
import gzip
import zlib
from bs4 import BeautifulSoup
from urllib.parse import quote

def analyze_response_details(url, test_name):
    """è©³ç´°åˆ†æ HTTP å›æ‡‰"""
    print(f"\n{'='*60}")
    print(f"ğŸ” åˆ†æ: {test_name}")
    print(f"ğŸ“‹ URL: {url}")
    print(f"{'='*60}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',  # æ”¯æ´å£“ç¸®
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        with httpx.Client(timeout=30, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            
            print(f"ğŸ“Š å›æ‡‰ç‹€æ…‹: {response.status_code}")
            print(f"ğŸ“ åŸå§‹å…§å®¹é•·åº¦: {len(response.content)} bytes")
            print(f"ğŸ”— æœ€çµ‚ URL: {response.url}")
            
            # æª¢æŸ¥å›æ‡‰æ¨™é ­
            print(f"\nğŸ“„ é‡è¦æ¨™é ­:")
            important_headers = ['content-type', 'content-encoding', 'transfer-encoding', 'content-length']
            for header in important_headers:
                value = response.headers.get(header, 'N/A')
                print(f"  {header}: {value}")
            
            # å˜—è©¦è§£å£“ç¸®å…§å®¹
            content = response.content
            print(f"\nğŸ—œï¸ å£“ç¸®æª¢æ¸¬:")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º gzip
            if content.startswith(b'\x1f\x8b'):
                print("  âœ… æª¢æ¸¬åˆ° gzip å£“ç¸®")
                try:
                    decompressed = gzip.decompress(content)
                    print(f"  ğŸ“ è§£å£“å¾Œé•·åº¦: {len(decompressed)} bytes")
                    content = decompressed
                except Exception as e:
                    print(f"  âŒ gzip è§£å£“å¤±æ•—: {e}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º deflate
            elif response.headers.get('content-encoding') == 'deflate':
                print("  âœ… æª¢æ¸¬åˆ° deflate å£“ç¸®")
                try:
                    decompressed = zlib.decompress(content)
                    print(f"  ğŸ“ è§£å£“å¾Œé•·åº¦: {len(decompressed)} bytes")
                    content = decompressed
                except Exception as e:
                    print(f"  âŒ deflate è§£å£“å¤±æ•—: {e}")
            else:
                print("  â„¹ï¸ æœªæª¢æ¸¬åˆ°å·²çŸ¥å£“ç¸®æ ¼å¼")
            
            # å˜—è©¦ UTF-8 è§£ç¢¼
            print(f"\nğŸ“ å…§å®¹åˆ†æ:")
            try:
                text_content = content.decode('utf-8', errors='replace')
                replacement_ratio = text_content.count('\ufffd') / len(text_content) if text_content else 1.0
                print(f"  UTF-8 è§£ç¢¼æ›¿æ›æ¯”ä¾‹: {replacement_ratio:.3f}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ HTML
                if '<html' in text_content.lower() and '</html>' in text_content.lower():
                    print("  âœ… åŒ…å«å®Œæ•´ HTML çµæ§‹")
                    
                    # è§£æ HTML
                    soup = BeautifulSoup(text_content, 'html.parser')
                    print(f"  ğŸ“„ è§£æåˆ° {len(soup.find_all())} å€‹ HTML æ¨™ç±¤")
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰ JavaScript é‡å®šå‘æˆ–å‹•æ…‹è¼‰å…¥
                    scripts = soup.find_all('script')
                    if scripts:
                        print(f"  ğŸ¯ æ‰¾åˆ° {len(scripts)} å€‹ script æ¨™ç±¤")
                        for i, script in enumerate(scripts[:3]):  # åªæª¢æŸ¥å‰3å€‹
                            script_text = script.get_text()
                            if 'window.location' in script_text or 'redirect' in script_text.lower():
                                print(f"    Script {i+1}: å¯èƒ½åŒ…å«é‡å®šå‘é‚è¼¯")
                            elif 'ajax' in script_text.lower() or 'fetch' in script_text.lower():
                                print(f"    Script {i+1}: å¯èƒ½åŒ…å«å‹•æ…‹è¼‰å…¥é‚è¼¯")
                    
                    # å°‹æ‰¾æœå°‹çµæœç›¸é—œçš„å…ƒç´ 
                    search_indicators = [
                        ('div', 'class', ['search-result', 'search-results', 'results']),
                        ('div', 'class', ['product', 'product-item', 'product-div']),
                        ('div', 'class', ['actress', 'actress-name', 'performer']),
                        ('div', 'id', ['search-results', 'content', 'main']),
                    ]
                    
                    print(f"  ğŸ” æœå°‹çµæœå…ƒç´ :")
                    for tag, attr, values in search_indicators:
                        for value in values:
                            elements = soup.find_all(tag, {attr: value})
                            if elements:
                                print(f"    âœ… {tag}[{attr}='{value}']: {len(elements)} å€‹")
                            
                    # é¡¯ç¤ºå‰ 500 å­—ç¬¦çš„å…§å®¹é è¦½
                    clean_text = ' '.join(text_content.split())[:500]
                    print(f"  ğŸ“– å…§å®¹é è¦½: {clean_text}...")
                    
                else:
                    print("  âŒ ä¸æ˜¯æœ‰æ•ˆçš„ HTML çµæ§‹")
                    # é¡¯ç¤ºåŸå§‹å…§å®¹çš„å‰ 200 å­—ç¬¦
                    preview = text_content[:200].replace('\n', ' ').replace('\r', ' ')
                    print(f"  ğŸ“„ åŸå§‹å…§å®¹é è¦½: {preview}...")
                    
            except Exception as e:
                print(f"  âŒ UTF-8 è§£ç¢¼å¤±æ•—: {e}")
                # é¡¯ç¤ºäºŒé€²åˆ¶å…§å®¹çš„åå…­é€²åˆ¶è¡¨ç¤º
                hex_preview = content[:50].hex()
                print(f"  ğŸ” åå…­é€²åˆ¶é è¦½: {hex_preview}")
                
    except Exception as e:
        print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")

def main():
    """ä¸»è¦æ¸¬è©¦å‡½å¼"""
    print("ğŸš€ HTTP å›æ‡‰è©³ç´°åˆ†æ")
    print("=" * 80)
    
    test_cases = [
        ("AV-WIKI SONE-553", "https://av-wiki.net/?s=SONE-553&post_type=product"),
        ("chiba-f.net SONE-553", "https://chiba-f.net/search/?keyword=SONE-553"),
        ("AV-WIKI é¦–é ", "https://av-wiki.net/"),
        ("chiba-f.net é¦–é ", "https://chiba-f.net/"),
    ]
    
    for test_name, url in test_cases:
        analyze_response_details(url, test_name)
        print(f"\nâ° ç­‰å¾… 3 ç§’...")
        import time
        time.sleep(3)
    
    print(f"\n{'='*80}")
    print("ğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
