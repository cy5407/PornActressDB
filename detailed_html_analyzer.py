# -*- coding: utf-8 -*-
"""
è©³ç´°HTMLå…§å®¹åˆ†æè…³æœ¬
æ·±å…¥åˆ†æç·¨ç¢¼å•é¡Œä¸¦æ‰¾å‡ºæœ€ä½³è§£æ±ºæ–¹æ¡ˆ
"""

import requests
import httpx
from bs4 import BeautifulSoup
import chardet
import logging
from typing import Dict, Any
import asyncio

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DetailedHTMLAnalyzer:
    """è©³ç´°HTMLåˆ†æå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,ja;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def find_best_encoding(self, content: bytes) -> str:
        """æ‰¾å‡ºæœ€ä½³ç·¨ç¢¼æ–¹å¼"""
        encodings_to_try = [
            'utf-8',
            'shift_jis', 
            'cp932',
            'euc-jp',
            'iso-2022-jp',
            'gb2312',
            'big5'
        ]
        
        best_encoding = 'utf-8'
        min_replacement_chars = float('inf')
        
        for encoding in encodings_to_try:
            try:
                decoded = content.decode(encoding, errors='replace')
                replacement_count = decoded.count('ï¿½')
                
                logger.info(f"   {encoding}: {replacement_count} å€‹æ›¿æ›å­—ç¬¦")
                
                if replacement_count < min_replacement_chars:
                    min_replacement_chars = replacement_count
                    best_encoding = encoding
                    
            except Exception as e:
                logger.warning(f"   {encoding}: è§£ç¢¼å¤±æ•— - {e}")
        
        logger.info(f"ğŸ¯ æœ€ä½³ç·¨ç¢¼: {best_encoding} (æ›¿æ›å­—ç¬¦: {min_replacement_chars})")
        return best_encoding
    
    def analyze_content_in_detail(self, url: str) -> Dict[str, Any]:
        """è©³ç´°åˆ†æå…§å®¹"""
        logger.info(f"\nğŸ” è©³ç´°åˆ†æ: {url}")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            logger.info(f"   ç‹€æ…‹ç¢¼: {response.status_code}")
            logger.info(f"   Content-Type: {response.headers.get('content-type', 'Unknown')}")
            logger.info(f"   å…§å®¹é•·åº¦: {len(response.content)} bytes")
            
            # æ‰¾å‡ºæœ€ä½³ç·¨ç¢¼
            best_encoding = self.find_best_encoding(response.content)
            
            # ä½¿ç”¨æœ€ä½³ç·¨ç¢¼è§£ç¢¼
            content = response.content.decode(best_encoding, errors='replace')
            
            # BeautifulSoup è§£æ
            soup = BeautifulSoup(content, 'html.parser')
            
            # åŸºæœ¬è³‡è¨Š
            title = soup.title.string if soup.title else "ç„¡æ¨™é¡Œ"
            logger.info(f"   é é¢æ¨™é¡Œ: {title}")
            
            # æª¢æŸ¥metaæ¨™ç±¤ä¸­çš„ç·¨ç¢¼è²æ˜
            charset_meta = soup.find('meta', {'charset': True})
            if charset_meta:
                declared_charset = charset_meta.get('charset')
                logger.info(f"   è²æ˜ç·¨ç¢¼ (meta charset): {declared_charset}")
            
            content_type_meta = soup.find('meta', {'http-equiv': 'Content-Type'})
            if content_type_meta:
                content_type = content_type_meta.get('content', '')
                logger.info(f"   è²æ˜ç·¨ç¢¼ (http-equiv): {content_type}")
            
            # åˆ†ææœå°‹çµæœçµæ§‹
            self.analyze_search_structure(soup, url)
            
            # å„²å­˜éƒ¨åˆ†å…§å®¹ä¾›æª¢è¦–
            preview_content = content[:2000]
            
            return {
                'url': url,
                'success': True,
                'best_encoding': best_encoding,
                'title': title,
                'content_length': len(content),
                'preview': preview_content
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±æ•—: {e}")
            return {'url': url, 'success': False, 'error': str(e)}
    
    def analyze_search_structure(self, soup: BeautifulSoup, url: str):
        """åˆ†ææœå°‹çµæœçµæ§‹"""
        logger.info("   ğŸ” æœå°‹çµæœçµæ§‹åˆ†æ:")
        
        # é€šç”¨æœå°‹çµæœå®¹å™¨
        possible_containers = [
            'div[class*="search"]',
            'div[class*="result"]', 
            'div[class*="product"]',
            'div[class*="item"]',
            'article',
            '.post',
            '.entry'
        ]
        
        found_containers = []
        for selector in possible_containers:
            elements = soup.select(selector)
            if elements:
                found_containers.append((selector, len(elements)))
                logger.info(f"     {selector}: {len(elements)} å€‹å…ƒç´ ")
        
        if not found_containers:
            logger.warning("     âš ï¸ æœªæ‰¾åˆ°æ˜é¡¯çš„æœå°‹çµæœå®¹å™¨")
            
            # å˜—è©¦æ‰¾ä»»ä½•åŒ…å«å½±ç‰‡ç·¨è™Ÿçš„å…ƒç´ 
            midv_elements = soup.find_all(text=lambda text: text and 'MIDV' in text.upper())
            if midv_elements:
                logger.info(f"     æ‰¾åˆ° {len(midv_elements)} å€‹åŒ…å« 'MIDV' çš„æ–‡å­—å…ƒç´ ")
                for i, elem in enumerate(midv_elements[:3]):
                    parent = elem.parent if elem.parent else None
                    parent_tag = f"{parent.name}#{parent.get('id', '')}.{parent.get('class', '')}" if parent else "ç„¡çˆ¶å…ƒç´ "
                    logger.info(f"       [{i+1}] {elem.strip()[:50]}... (çˆ¶å…ƒç´ : {parent_tag})")
        
        # åˆ†æé€£çµ
        links = soup.find_all('a', href=True)
        relevant_links = [link for link in links if 'midv' in link.get('href', '').lower() or 'midv' in link.get_text().lower()]
        
        if relevant_links:
            logger.info(f"     æ‰¾åˆ° {len(relevant_links)} å€‹ç›¸é—œé€£çµ:")
            for i, link in enumerate(relevant_links[:3]):
                href = link.get('href', '')
                text = link.get_text(strip=True)[:50]
                logger.info(f"       [{i+1}] {text}... -> {href}")
        else:
            logger.warning("     âš ï¸ æœªæ‰¾åˆ°ç›¸é—œé€£çµ")

def run_detailed_analysis():
    """åŸ·è¡Œè©³ç´°åˆ†æ"""
    analyzer = DetailedHTMLAnalyzer()
    
    test_urls = [
        'https://av-wiki.net/?s=MIDV-661&post_type=product',
        'https://chiba-f.net/search/?keyword=MIDV-661'
    ]
    
    logger.info("ğŸ¯ é–‹å§‹è©³ç´°HTMLå…§å®¹åˆ†æ")
    logger.info("=" * 80)
    
    results = []
    for url in test_urls:
        result = analyzer.analyze_content_in_detail(url)
        results.append(result)
    
    # ç”¢ç”Ÿåˆ†æå ±å‘Š
    logger.info("\nğŸ“Š åˆ†æç¸½çµ:")
    for result in results:
        if result.get('success'):
            logger.info(f"âœ… {result['url']}")
            logger.info(f"   æœ€ä½³ç·¨ç¢¼: {result['best_encoding']}")
            logger.info(f"   å…§å®¹é•·åº¦: {result['content_length']}")
        else:
            logger.error(f"âŒ {result['url']}: {result.get('error')}")
    
    return results

def save_content_samples(results):
    """å„²å­˜å…§å®¹æ¨£æœ¬ä¾›æª¢è¦–"""
    for i, result in enumerate(results):
        if result.get('success') and result.get('preview'):
            filename = f"content_sample_{i+1}.html"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"<!-- URL: {result['url']} -->\n")
                f.write(f"<!-- Best Encoding: {result['best_encoding']} -->\n")
                f.write(f"<!-- Title: {result['title']} -->\n\n")
                f.write(result['preview'])
            logger.info(f"ğŸ’¾ å·²å„²å­˜å…§å®¹æ¨£æœ¬: {filename}")

if __name__ == "__main__":
    results = run_detailed_analysis()
    save_content_samples(results)
