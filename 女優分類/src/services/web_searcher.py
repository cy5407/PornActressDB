# -*- coding: utf-8 -*-
"""
ç¶²è·¯æœå°‹å™¨æ¨¡çµ„
"""
import re
import time
import logging
import threading
import concurrent.futures
from typing import Dict, List, Optional, Callable, Any
import httpx
from bs4 import BeautifulSoup
from urllib.parse import quote

from ..models.config import ConfigManager
from .safe_searcher import SafeSearcher, RequestConfig
from .safe_javdb_searcher import SafeJAVDBSearcher
from ..models.results import Result, ServiceError, ErrorCode

logger = logging.getLogger(__name__)


class WebSearcher:
    """å¢å¼·ç‰ˆæœå°‹å™¨ - æ”¯æ´æœå°‹çµæœé é¢"""

    def __init__(self, config: ConfigManager):
        # åˆå§‹åŒ–å®‰å…¨æœå°‹å™¨é…ç½®
        safe_config = RequestConfig(
            min_interval=config.getfloat("search", "min_interval", fallback=1.0),
            max_interval=config.getfloat("search", "max_interval", fallback=3.0),
            enable_cache=config.getboolean("search", "enable_cache", fallback=True),
            cache_duration=config.getint("search", "cache_duration", fallback=86400),
            max_retries=config.getint("search", "max_retries", fallback=3),
            backoff_factor=config.getfloat("search", "backoff_factor", fallback=2.0),
            rotate_headers=config.getboolean("search", "rotate_headers", fallback=True),
        )

        # åˆå§‹åŒ–æ—¥æ–‡ç¶²ç«™å°ˆç”¨çš„æ›´å¿«é€Ÿé…ç½®ï¼ˆav-wiki å’Œ chiba-f æ¯”è¼ƒä¸æœƒæ“‹çˆ¬èŸ²ï¼‰
        japanese_config = RequestConfig(
            min_interval=config.getfloat(
                "search", "japanese_min_interval", fallback=0.5
            ),
            max_interval=config.getfloat(
                "search", "japanese_max_interval", fallback=1.5
            ),
            enable_cache=config.getboolean("search", "enable_cache", fallback=True),
            cache_duration=config.getint("search", "cache_duration", fallback=86400),
            max_retries=config.getint("search", "max_retries", fallback=3),
            backoff_factor=config.getfloat("search", "backoff_factor", fallback=1.5),
            rotate_headers=config.getboolean("search", "rotate_headers", fallback=True),
        )
        # åˆå§‹åŒ–å®‰å…¨æœå°‹å™¨
        self.safe_searcher = SafeSearcher(safe_config)

        # ç‚ºæ—¥æ–‡ç¶²ç«™å»ºç«‹æ›´å¿«é€Ÿçš„æœå°‹å™¨ï¼ˆav-wiki å’Œ chiba-f æ¯”è¼ƒä¸æœƒæ“‹çˆ¬èŸ²ï¼‰
        japanese_config = RequestConfig(
            min_interval=0.5,  # æ—¥æ–‡ç¶²ç«™è¼ƒçŸ­å»¶é²
            max_interval=1.5,
            enable_cache=True,
            cache_duration=86400,
            max_retries=3,
            backoff_factor=1.5,
            rotate_headers=True,
        )
        self.japanese_searcher = SafeSearcher(japanese_config)

        # åˆå§‹åŒ– JAVDB å®‰å…¨æœå°‹å™¨
        cache_dir = config.get("search", "cache_dir", fallback=None)
        self.javdb_searcher = SafeJAVDBSearcher(cache_dir)
        # ä¿ç•™åŸæœ‰é…ç½®ä»¥å‘ä¸‹ç›¸å®¹
        self.headers = self.safe_searcher.get_headers()

        # ğŸ”§ æ—¥æ–‡ç¶²ç«™å°ˆç”¨æ¨™é ­ï¼ˆè§£æ±º Brotli å£“ç¸®å•é¡Œï¼‰
        self.japanese_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
            # é—œéµï¼šä¸åŒ…å« Accept-Encoding ä»¥é¿å… Brotli å•é¡Œ
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        self.search_cache = {}
        self.batch_size = config.getint("search", "batch_size", fallback=10)
        self.thread_count = config.getint("search", "thread_count", fallback=5)
        self.batch_delay = config.getfloat("search", "batch_delay", fallback=2.0)
        self.timeout = config.getint("search", "request_timeout", fallback=20)
        logger.info("ğŸ›¡ï¸ å·²å•Ÿç”¨å®‰å…¨æœå°‹å™¨åŠŸèƒ½")
        logger.info("ğŸ‡¯ğŸ‡µ å·²å•Ÿç”¨æ—¥æ–‡ç¶²ç«™å¿«é€Ÿæœå°‹åŠŸèƒ½")
        logger.info("ğŸ¬ å·²å•Ÿç”¨ JAVDB å®‰å…¨æœå°‹åŠŸèƒ½")

    def search_info(self, code: str, stop_event: threading.Event) -> Result[Dict]:
        """å¤šå±¤ç´šæœå°‹ç­–ç•¥ - AV-WIKI -> chiba-f.net -> JAVDB"""
        if stop_event.is_set():
            return Result.fail(ServiceError(ErrorCode.VALIDATION_ERROR, "ä»»å‹™è¢«ä¸­æ­¢"))
        if code in self.search_cache:
            return Result.ok(self.search_cache[code])

        try:
            # ç¬¬ä¸€å±¤ï¼šåŸæœ‰çš„ AV-WIKI æœå°‹
            logger.debug(f"ğŸ” ç¬¬ä¸€å±¤æœå°‹ - AV-WIKI: {code}")
            result = self._search_av_wiki(code, stop_event)
            if result.success and result.data and result.data.get("actresses"):
                self.search_cache[code] = result.data
                return result

            # ç¬¬äºŒå±¤ï¼šchiba-f.net æœå°‹
            if not stop_event.is_set():
                logger.debug(f"ğŸ” ç¬¬äºŒå±¤æœå°‹ - chiba-f.net: {code}")
                result = self._search_chiba_f_net(code, stop_event)
                if result.success and result.data and result.data.get("actresses"):
                    self.search_cache[code] = result.data
                    return result

            # ç¬¬ä¸‰å±¤ï¼šä½¿ç”¨å®‰å…¨çš„ JAVDB æœå°‹
            if not stop_event.is_set():
                logger.debug(f"ğŸ” ç¬¬ä¸‰å±¤æœå°‹ - JAVDB: {code}")
                javdb_result = self.javdb_searcher.search_javdb(code)
                if (
                    javdb_result.success
                    and javdb_result.data
                    and javdb_result.data.get("actresses")
                ):
                    # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                    result_data = {
                        "source": javdb_result.data["source"],
                        "actresses": javdb_result.data["actresses"],
                        "studio": javdb_result.data.get("studio"),
                        "studio_code": javdb_result.data.get("studio_code"),
                        "release_date": javdb_result.data.get("release_date"),
                        "title": javdb_result.data.get("title"),
                        "duration": javdb_result.data.get("duration"),
                        "director": javdb_result.data.get("director"),
                        "series": javdb_result.data.get("series"),
                        "rating": javdb_result.data.get("rating"),
                        "categories": javdb_result.data.get("categories", []),
                    }
                    self.search_cache[code] = result_data

                    # è±å¯Œçš„æ—¥èªŒè¼¸å‡º
                    log_parts = [f"ç•ªè™Ÿ {code} é€é {result_data['source']} æ‰¾åˆ°:"]
                    log_parts.append(f"å¥³å„ª: {', '.join(result_data['actresses'])}")
                    log_parts.append(f"ç‰‡å•†: {result_data.get('studio', 'æœªçŸ¥')}")

                    if result_data.get("rating"):
                        log_parts.append(f"è©•åˆ†: {result_data['rating']}")
                    if result_data.get("categories"):
                        categories_str = ", ".join(
                            result_data["categories"][:3]
                        )  # åªé¡¯ç¤ºå‰3å€‹é¡åˆ¥
                        if len(result_data["categories"]) > 3:
                            categories_str += (
                                f" ç­‰{len(result_data['categories'])}å€‹é¡åˆ¥"
                            )
                        log_parts.append(f"é¡åˆ¥: {categories_str}")

                    logger.info(" | ".join(log_parts))
                    return Result.ok(result_data)

            logger.warning(f"ç•ªè™Ÿ {code} æœªåœ¨æ‰€æœ‰æœå°‹æºä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚")
            return Result.fail(
                ServiceError(
                    ErrorCode.PARSING_ERROR,
                    f"ç•ªè™Ÿ {code} æœªåœ¨æ‰€æœ‰æœå°‹æºä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚",
                )
            )
        except Exception as e:
            logger.error(f"æœå°‹ç•ªè™Ÿ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return Result.fail(
                ServiceError(
                    ErrorCode.UNKNOWN_ERROR,
                    f"æœå°‹ç•ªè™Ÿ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}",
                    caused_by=e,
                )
            )

    def _search_av_wiki(self, code: str, stop_event: threading.Event) -> Result[Dict]:
        """AV-WIKI æœå°‹æ–¹æ³•"""
        if stop_event.is_set():
            return Result.fail(ServiceError(ErrorCode.VALIDATION_ERROR, "ä»»å‹™è¢«ä¸­æ­¢"))

        search_url = f"https://av-wiki.net/?s={quote(code)}&post_type=product"

        # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æ¨™é ­ï¼ˆè§£æ±º Brotli å£“ç¸®å•é¡Œï¼‰
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                # ğŸ”§ ä½¿ç”¨ä¸æ”¯æ´å£“ç¸®çš„æ¨™é ­ï¼Œé¿å… Brotli å•é¡Œ
                response = client.get(url, headers=self.japanese_headers)
                response.raise_for_status()
                # ç›´æ¥ä½¿ç”¨ response.textï¼Œå› ç‚ºæˆ‘å€‘å·²ç¶“ç¦ç”¨äº†å£“ç¸®
                return BeautifulSoup(response.text, "html.parser")

        try:
            soup_result = self.safe_searcher.safe_request(make_request, search_url)
            if not soup_result.success or soup_result.data is None:
                logger.warning(
                    f"ç„¡æ³•ç²å– {code} çš„ AV-WIKI æœå°‹é é¢: {soup_result.error.message if soup_result.error else 'æœªçŸ¥éŒ¯èª¤'}"
                )
                return Result.fail(
                    ServiceError(
                        ErrorCode.NETWORK_ERROR,
                        f"ç„¡æ³•ç²å– {code} çš„ AV-WIKI æœå°‹é é¢",
                        {"url": search_url},
                    )
                )

            soup = soup_result.data

            # å…ˆæª¢æŸ¥æ˜¯å¦æœ‰æœå°‹çµæœ
            search_results = soup.find_all("div", class_="column-flex")
            logger.info(f"AV-WIKI æœå°‹ {code}: æ‰¾åˆ° {len(search_results)} å€‹æœå°‹çµæœ")

            if not search_results:
                # æª¢æŸ¥æ˜¯å¦æ˜¯ "æ²’æœ‰æ‰¾åˆ°çµæœ" çš„é é¢
                no_results_indicators = [
                    "è©²å½“ãªã—",
                    "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                    "æ¤œç´¢çµæœï¼š0",
                    "0ä»¶",
                ]
                page_text = soup.get_text()
                for indicator in no_results_indicators:
                    if indicator in page_text:
                        logger.info(f"AV-WIKI æ˜ç¢ºé¡¯ç¤ºæ²’æœ‰æ‰¾åˆ° {code} çš„çµæœ")
                        return Result.fail(
                            ServiceError(
                                ErrorCode.PARSING_ERROR,
                                f"AV-WIKI æ˜ç¢ºé¡¯ç¤ºæ²’æœ‰æ‰¾åˆ° {code} çš„çµæœ",
                            )
                        )

            # æ­£ç¢ºè§£æå¥³å„ªåç¨±ï¼š<li class="actress-name"><a>å¥³å„ªåç¨±</a></li>
            actress_elements = soup.find_all("li", class_="actress-name")
            actresses = []
            logger.info(
                f"AV-WIKI è§£æ: æ‰¾åˆ° {len(actress_elements)} å€‹ actress-name å…ƒç´ "
            )
            for li in actress_elements:
                link = li.find("a")
                if link and link.text.strip():
                    actress_name = link.text.strip()
                    actresses.append(actress_name)
                    logger.info(f"AV-WIKI æå–åˆ°å¥³å„ªåç¨±: {actress_name}")

            if not actresses:
                logger.warning(
                    f"AV-WIKI æœªæ‰¾åˆ°å¥³å„ªåç¨±ï¼ŒHTMLé–‹é ­: {str(soup)[:200]}..."
                )

            # æœå°‹ç‰‡å•†è³‡è¨Š
            studio_info = self._extract_studio_info(soup, code)

            if not actresses:
                page_text = soup.get_text()
                lines = [line.strip() for line in page_text.split("\n") if line.strip()]
                for i, line in enumerate(lines):
                    if code in line:
                        for j in range(max(0, i - 3), min(len(lines), i + 1)):
                            potential_name = lines[j].strip()
                            if potential_name and self._is_actress_name(potential_name):
                                if potential_name not in actresses:
                                    actresses.append(potential_name)
            if actresses:
                result = {
                    "source": "AV-WIKI (å®‰å…¨å¢å¼·ç‰ˆ)",
                    "actresses": actresses,
                    "studio": studio_info.get("studio"),
                    "studio_code": studio_info.get("studio_code"),
                    "release_date": studio_info.get("release_date"),
                }
                logger.info(
                    f"ç•ªè™Ÿ {code} é€é {result['source']} æ‰¾åˆ°: {', '.join(result['actresses'])}, ç‰‡å•†: {result.get('studio', 'æœªçŸ¥')}"
                )
                return Result.ok(result)

        except Exception as e:
            logger.error(f"AV-WIKI æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return Result.fail(
                ServiceError(
                    ErrorCode.UNKNOWN_ERROR,
                    f"AV-WIKI æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}",
                    caused_by=e,
                )
            )

        return Result.fail(
            ServiceError(ErrorCode.PARSING_ERROR, f"AV-WIKI æœªæ‰¾åˆ° {code} çš„å¥³å„ªè³‡è¨Š")
        )

    def _is_actress_name(self, text: str) -> bool:
        """åˆ¤æ–·æ–‡å­—æ˜¯å¦å¯èƒ½æ˜¯å¥³å„ªåç¨±"""
        if not text or len(text) < 2 or len(text) > 20:
            return False
        exclude_keywords = [
            "SOD",
            "STARS",
            "FANZA",
            "MGS",
            "MIDV",
            "SSIS",
            "IPX",
            "IPZZ",
            "ç¶šãã‚’èª­ã‚€",
            "æ¤œç´¢",
            "ä»¶",
            "ç‰¹å…¸",
            "æ˜ åƒ",
            "ä»˜ã",
            "star",
            "SOKMIL",
            "Menu",
            "ã‚»ãƒ¼ãƒ«",
            "é™å®š",
            "æœ€å¤§",
        ]
        if any(keyword in text for keyword in exclude_keywords):
            return False
        if re.match(r"^\d+$", text) or len(re.findall(r"\d", text)) > len(text) // 2:
            return False
        if re.search(r"[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]", text):
            return True
        return False

    def batch_search(
        self,
        items: List,
        task_func: Callable,
        stop_event: threading.Event,
        progress_callback=None,
    ) -> Dict[str, Result]:
        results = {}
        total_batches = (len(items) + self.batch_size - 1) // self.batch_size
        for i in range(0, len(items), self.batch_size):
            if stop_event.is_set():
                logger.info("ä»»å‹™è¢«ä½¿ç”¨è€…ä¸­æ­¢ã€‚")
                break
            batch = items[i : i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            if progress_callback:
                progress_callback(f"è™•ç†æ‰¹æ¬¡ {batch_num}/{total_batches}...\n")
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.thread_count
            ) as executor:
                future_to_item = {
                    executor.submit(task_func, item, stop_event): item for item in batch
                }
                for future in concurrent.futures.as_completed(future_to_item):
                    if stop_event.is_set():
                        break
                    item = future_to_item[future]
                    try:
                        result = future.result()
                        results[item] = result
                        if progress_callback:
                            if (
                                result.success
                                and result.data
                                and result.data.get("actresses")
                            ):
                                progress_callback(f"âœ… {item}: æ‰¾åˆ°è³‡æ–™\n")
                            else:
                                progress_callback(
                                    f"âŒ {item}: æœªæ‰¾åˆ°çµæœæˆ–æœå°‹å¤±æ•— - {result.error.message if result.error else 'æœªçŸ¥éŒ¯èª¤'}\n"
                                )
                    except Exception as e:
                        logger.error(f"æ‰¹æ¬¡è™•ç† {item} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        results[item] = Result.fail(
                            ServiceError(
                                ErrorCode.UNKNOWN_ERROR,
                                f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}",
                                caused_by=e,
                            )
                        )
                        if progress_callback:
                            progress_callback(f"ğŸ’¥ {item}: è™•ç†å¤±æ•— - {e}\n")
            if i + self.batch_size < len(items) and total_batches > 1:
                time.sleep(self.batch_delay)
        return results

    def _search_chiba_f_net(
        self, code: str, stop_event: threading.Event
    ) -> Result[Dict]:
        """ä½¿ç”¨ chiba-f.net æœå°‹å¥³å„ªè³‡è¨Š"""
        if stop_event.is_set():
            return Result.fail(ServiceError(ErrorCode.VALIDATION_ERROR, "ä»»å‹™è¢«ä¸­æ­¢"))

        search_url = f"https://chiba-f.net/search/?keyword={quote(code)}"

        # ä½¿ç”¨æ—¥æ–‡ç¶²ç«™å°ˆç”¨æ¨™é ­ï¼ˆè§£æ±º Brotli å£“ç¸®å•é¡Œï¼‰
        def make_request(url, **kwargs):
            with httpx.Client(timeout=self.timeout, **kwargs) as client:
                # ğŸ”§ ä½¿ç”¨ä¸æ”¯æ´å£“ç¸®çš„æ¨™é ­ï¼Œé¿å… Brotli å•é¡Œ
                response = client.get(url, headers=self.japanese_headers)
                response.raise_for_status()
                # ç›´æ¥ä½¿ç”¨ response.textï¼Œå› ç‚ºæˆ‘å€‘å·²ç¶“ç¦ç”¨äº†å£“ç¸®
                return BeautifulSoup(response.text, "html.parser")

        try:
            soup_result = self.safe_searcher.safe_request(make_request, search_url)
            if not soup_result.success or soup_result.data is None:
                logger.warning(
                    f"ç„¡æ³•ç²å– {code} çš„ chiba-f.net æœå°‹é é¢: {soup_result.error.message if soup_result.error else 'æœªçŸ¥éŒ¯èª¤'}"
                )
                return Result.fail(
                    ServiceError(
                        ErrorCode.NETWORK_ERROR,
                        f"ç„¡æ³•ç²å– {code} çš„ chiba-f.net æœå°‹é é¢",
                        {"url": search_url},
                    )
                )

            soup = soup_result.data

            # æŸ¥æ‰¾ç”¢å“å€å¡Š
            product_divs = soup.find_all("div", class_="product-div")
            logger.info(
                f"chiba-f.net è§£æ: æ‰¾åˆ° {len(product_divs)} å€‹ product-div å…ƒç´ "
            )

            for product_div in product_divs:
                # æª¢æŸ¥ç•ªè™Ÿæ˜¯å¦åŒ¹é…
                pno_element = product_div.find("div", class_="pno")
                if pno_element and code.upper() in pno_element.text.upper():
                    logger.info(f"chiba-f.net æ‰¾åˆ°åŒ¹é…ç•ªè™Ÿ: {code}")
                    extracted_info = self._extract_chiba_product_info(product_div, code)
                    if extracted_info:
                        return Result.ok(extracted_info)

            # å¦‚æœæ²’æœ‰æ‰¾åˆ°å®Œå…¨åŒ¹é…ï¼Œå˜—è©¦æ¨¡ç³ŠåŒ¹é…
            for product_div in product_divs:
                product_text = product_div.get_text()
                if code.upper() in product_text.upper():
                    logger.info(f"chiba-f.net æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ç•ªè™Ÿ: {code}")
                    extracted_info = self._extract_chiba_product_info(product_div, code)
                    if extracted_info:
                        return Result.ok(extracted_info)

            if not product_divs:
                logger.warning(
                    f"chiba-f.net æœªæ‰¾åˆ°ä»»ä½•ç”¢å“å€å¡Šï¼ŒHTMLé–‹é ­: {str(soup)[:200]}..."
                )

        except Exception as e:
            logger.error(f"chiba-f.net æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return Result.fail(
                ServiceError(
                    ErrorCode.UNKNOWN_ERROR,
                    f"chiba-f.net æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}",
                    caused_by=e,
                )
            )

        logger.debug(f"ç•ªè™Ÿ {code} æœªåœ¨ chiba-f.net ä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚")
        return Result.fail(
            ServiceError(
                ErrorCode.PARSING_ERROR,
                f"ç•ªè™Ÿ {code} æœªåœ¨ chiba-f.net ä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚",
            )
        )

    def _extract_chiba_product_info(self, product_div, code: str) -> Optional[Dict]:
        """å¾ chiba-f.net ç”¢å“å€å¡Šæå–è³‡è¨Š"""
        result = {
            "source": "chiba-f.net (å®‰å…¨å¢å¼·ç‰ˆ)",
            "actresses": [],
            "studio": None,
            "studio_code": None,
            "release_date": None,
        }

        try:
            # æå–å¥³å„ªåç¨±
            actress_span = product_div.find("span", class_="fw-bold")
            if actress_span:
                result["actresses"] = [actress_span.text.strip()]

            # æå–ç³»åˆ—/ç‰‡å•†è³‡è¨Š
            series_link = product_div.find("a", href=re.compile(r"../series/"))
            if series_link:
                result["studio"] = series_link.text.strip()
                # å¾ href æå–ç‰‡å•†ä»£ç¢¼
                href = series_link.get("href", "")
                if "../series/" in href:
                    result["studio_code"] = href.replace("../series/", "").strip()

            # æå–ç™¼è¡Œæ—¥æœŸ
            date_span = product_div.find("span", class_="start_date")
            if date_span:
                result["release_date"] = date_span.text.strip()

            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰‡å•†ï¼Œå˜—è©¦å¾ç•ªè™Ÿæ¨æ¸¬
            if not result["studio_code"]:
                result["studio_code"] = self._extract_studio_code_from_number(code)

            if result["actresses"]:
                self.search_cache[code] = result
                logger.info(
                    f"ç•ªè™Ÿ {code} é€é {result['source']} æ‰¾åˆ°: {', '.join(result['actresses'])}, ç‰‡å•†: {result.get('studio', 'æœªçŸ¥')}"
                )
                return result

        except Exception as e:
            logger.warning(f"æå– {code} å¾ chiba-f.net è³‡è¨Šæ™‚ç™¼ç”Ÿéƒ¨åˆ†éŒ¯èª¤: {str(e)}")

        return None

    def _extract_studio_info(self, soup: BeautifulSoup, code: str) -> Dict:
        """å¾ç¶²é ä¸­æå–ç‰‡å•†è³‡è¨Š"""
        studio_info = {"studio": None, "studio_code": None, "release_date": None}

        try:
            # å…ˆå–å¾—ç¶²é æ–‡å­—å…§å®¹ï¼Œå¾ŒçºŒæ–¹æ³•éƒ½å¯èƒ½ç”¨åˆ°
            page_text = soup.get_text()

            # æ–¹æ³•1: å¾ AV-WIKI HTML çµæ§‹ä¸­ç›´æ¥æå–ç‰‡å•†è³‡è¨Š
            # æŸ¥æ‰¾åŒ…å« fa-clone åœ–æ¨™çš„ li å…ƒç´ 
            studio_elements = soup.find_all("li")
            for li in studio_elements:
                icon = li.find("i", class_="fa-clone")
                if icon:
                    link = li.find("a")
                    if link and link.text.strip():
                        studio_text = link.text.strip()
                        # è§£æç‰‡å•†åç¨±ï¼Œä¾‹å¦‚ "ã‚¨ã‚¹ãƒ¯ãƒ³ - SONE" -> studio="ã‚¨ã‚¹ãƒ¯ãƒ³", code="SONE"
                        if " - " in studio_text:
                            parts = studio_text.split(" - ")
                            studio_info["studio"] = parts[0].strip()
                            studio_info["studio_code"] = parts[1].strip()
                        else:
                            studio_info["studio"] = studio_text
                        break

            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±æ•—ï¼Œå˜—è©¦å¾ç•ªè™Ÿä¸­æå–ç‰‡å•†ä»£ç¢¼
            if not studio_info["studio"]:
                studio_code = self._extract_studio_code_from_number(code)
                if studio_code:
                    studio_info["studio_code"] = studio_code
                    studio_info["studio"] = self._get_studio_name_by_code(studio_code)

            # æ–¹æ³•3: å¾ç¶²é å…§å®¹ä¸­æœå°‹ç‰‡å•†è³‡è¨Šï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
            if not studio_info["studio"]:
                # æœå°‹å¸¸è¦‹çš„ç‰‡å•†åç¨±å’Œæ¨¡å¼
                studio_patterns = [
                    # ç›´æ¥ç‰‡å•†åç¨±åŒ¹é…
                    (
                        r"(S1|SOD|MOODYZ|PREMIUM|WANZ|FALENO|ATTACKERS|E-BODY|KAWAII|FITCH|MADONNA|PRESTIGE)",
                        r"\1",
                    ),
                    # è£½ä½œå…¬å¸/ç™¼è¡Œå•†æ¨¡å¼
                    (r"è£½ä½œ[ï¼š:]\s*([^\n\r]+)", r"\1"),
                    (r"ç™¼è¡Œ[ï¼š:]\s*([^\n\r]+)", r"\1"),
                    (r"ãƒ¡ãƒ¼ã‚«ãƒ¼[ï¼š:]\s*([^\n\r]+)", r"\1"),
                    # ç•ªè™Ÿå‰ç¶´æ¨¡å¼
                    (r"å“ç•ª[ï¼š:]\s*([A-Z]+)-?\d+", r"\1"),
                ]

                for pattern, replacement in studio_patterns:
                    match = re.search(pattern, page_text, re.IGNORECASE)
                    if match:
                        extracted_studio = match.group(1).strip()
                        if (
                            extracted_studio and len(extracted_studio) < 50
                        ):  # åˆç†é•·åº¦é™åˆ¶
                            if not studio_info["studio"]:
                                studio_info["studio"] = extracted_studio
                            if (
                                not studio_info["studio_code"]
                                and len(extracted_studio) <= 10
                            ):
                                studio_info["studio_code"] = extracted_studio
                            break

            # æ–¹æ³•4: å˜—è©¦æå–ç™¼è¡Œæ—¥æœŸï¼ˆç¸½æ˜¯åŸ·è¡Œï¼‰
            date_patterns = [
                r"ç™¼å”®æ—¥[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})",
                r"(\d{4}[-/]\d{1,2}[-/]\d{1,2})",
                r"(\d{4}\.\d{1,2}\.\d{1,2})",
            ]

            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    studio_info["release_date"] = match.group(1)
                    break
        except Exception as e:
            logger.warning(f"æå–ç‰‡å•†è³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        return studio_info

    def _extract_studio_code_from_number(self, code: str) -> Optional[str]:
        """å¾ç•ªè™Ÿä¸­æå–ç‰‡å•†ä»£ç¢¼"""
        if not code:
            return None

        # æå–å­—æ¯éƒ¨åˆ†ä½œç‚ºç‰‡å•†ä»£ç¢¼
        match = re.match(r"^([A-Z]+)", code.upper())
        if match:
            return match.group(1)
        return None

    def _get_studio_name_by_code(self, studio_code: str) -> Optional[str]:
        """æ ¹æ“šç‰‡å•†ä»£ç¢¼ç²å–ç‰‡å•†åç¨±ï¼ˆå¾ studios.json è¼‰å…¥ï¼‰"""
        try:
            import json
            from pathlib import Path

            # è¼‰å…¥ studios.json æª”æ¡ˆ
            studios_file = Path(__file__).parent.parent.parent / "studios.json"
            if studios_file.exists():
                with open(studios_file, "r", encoding="utf-8") as f:
                    studios_data = json.load(f)

                # åå‘æŸ¥æ‰¾ï¼šå¾ä»£ç¢¼æ‰¾åˆ°ç‰‡å•†
                studio_code_upper = studio_code.upper()
                for studio_name, codes in studios_data.items():
                    if studio_code_upper in [code.upper() for code in codes]:
                        return studio_name
        except Exception as e:
            logger.warning(f"è¼‰å…¥ studios.json å¤±æ•—: {e}")

        # å›é€€åˆ°å…§å»ºå°æ‡‰è¡¨
        studio_mapping = {
            "STAR": "SOD",
            "STARS": "SOD",
            "SDJS": "SOD",
            "SSIS": "S1",
            "SSNI": "S1",
            "IPX": "IdeaPocket",
            "IPZZ": "IdeaPocket",
            "MIDV": "MOODYZ",
            "MIAA": "MOODYZ",
            "WANZ": "WANZ FACTORY",
            "FSDSS": "FALENO",
            "PRED": "PREMIUM",
            "ABW": "Prestige",
            "BF": "BeFree",
            "CAWD": "kawaii",
            "JUFD": "Fitch",
            "JUL": "MADONNA",
            "JUY": "MADONNA",
        }

        return studio_mapping.get(studio_code.upper(), studio_code)

    def get_safe_searcher_stats(self) -> Dict:
        """ç²å–å®‰å…¨æœå°‹å™¨çµ±è¨ˆè³‡è¨Š"""
        return self.safe_searcher.get_stats()

    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.search_cache.clear()
        self.safe_searcher.clear_cache()
        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰æœå°‹å¿«å–")

    def configure_safe_searcher(self, **kwargs):
        """å‹•æ…‹é…ç½®å®‰å…¨æœå°‹å™¨"""
        self.safe_searcher.configure(**kwargs)
        # æ›´æ–°æœ¬åœ°æ¨™é ­
        self.headers = self.safe_searcher.get_headers()

    def get_javdb_stats(self) -> Dict:
        """ç²å– JAVDB æœå°‹å™¨çµ±è¨ˆè³‡è¨Š"""
        return self.javdb_searcher.get_stats()

    def get_all_search_stats(self) -> Dict:
        """ç²å–æ‰€æœ‰æœå°‹å™¨çš„çµ±è¨ˆè³‡è¨Š"""
        return {
            "safe_searcher": self.get_safe_searcher_stats(),
            "javdb_searcher": self.get_javdb_stats(),
            "local_cache_entries": len(self.search_cache),
        }

    def clear_all_cache(self):
        """æ¸…ç©ºæ‰€æœ‰æœå°‹å¿«å–"""
        self.search_cache.clear()
        self.safe_searcher.clear_cache()
        self.javdb_searcher.clear_cache()
        logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰æœå°‹å¿«å– (åŒ…å« JAVDB)")

    def search_japanese_sites_only(
        self, code: str, stop_event: threading.Event
    ) -> Result[Dict]:
        """åƒ…æœå°‹æ—¥æ–‡ç¶²ç«™ - AV-WIKI å’Œ chiba-f.net"""
        if stop_event.is_set():
            return Result.fail(ServiceError(ErrorCode.VALIDATION_ERROR, "ä»»å‹™è¢«ä¸­æ­¢"))
        if code in self.search_cache:
            return Result.ok(self.search_cache[code])

        try:
            # ç¬¬ä¸€å±¤ï¼šAV-WIKI æœå°‹
            logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - AV-WIKI: {code}")
            result = self._search_av_wiki(code, stop_event)
            if result.success and result.data and result.data.get("actresses"):
                self.search_cache[code] = result.data
                return result

            # ç¬¬äºŒå±¤ï¼šchiba-f.net æœå°‹
            if not stop_event.is_set():
                logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - chiba-f.net: {code}")
                result = self._search_chiba_f_net(code, stop_event)
                if result.success and result.data and result.data.get("actresses"):
                    self.search_cache[code] = result.data
                    return result

            logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœªæ‰¾åˆ°: {code}")
            return Result.fail(
                ServiceError(ErrorCode.PARSING_ERROR, f"æ—¥æ–‡ç¶²ç«™æœªæ‰¾åˆ° {code}")
            )

        except Exception as e:
            logger.error(f"æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return Result.fail(
                ServiceError(
                    ErrorCode.UNKNOWN_ERROR,
                    f"æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}",
                    caused_by=e,
                )
            )

    def search_javdb_only(self, code: str, stop_event: threading.Event) -> Result[Dict]:
        """åƒ…æœå°‹ JAVDB"""
        if stop_event.is_set():
            return Result.fail(ServiceError(ErrorCode.VALIDATION_ERROR, "ä»»å‹™è¢«ä¸­æ­¢"))
        if code in self.search_cache:
            return Result.ok(self.search_cache[code])

        try:
            logger.debug(f"ğŸ“Š JAVDB æœå°‹: {code}")
            javdb_result = self.javdb_searcher.search_javdb(code)
            if (
                javdb_result.success
                and javdb_result.data is not None
                and isinstance(javdb_result.data, dict)
                and "actresses" in javdb_result.data
            ):
                # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                result_data = {
                    "source": javdb_result.data["source"],
                    "actresses": javdb_result.data["actresses"],
                    "studio": javdb_result.data.get("studio"),
                    "studio_code": javdb_result.data.get("studio_code"),
                    "release_date": javdb_result.data.get("release_date"),
                    "title": javdb_result.data.get("title"),
                    "duration": javdb_result.data.get("duration"),
                    "director": javdb_result.data.get("director"),
                    "series": javdb_result.data.get("series"),
                    "rating": javdb_result.data.get("rating"),
                    "categories": javdb_result.data.get("categories", []),
                }
                self.search_cache[code] = result_data
                return Result.ok(result_data)

            logger.debug(f"ğŸ“Š JAVDB æœªæ‰¾åˆ°: {code}")
            return Result.fail(
                ServiceError(ErrorCode.PARSING_ERROR, f"JAVDB æœªæ‰¾åˆ° {code}")
            )

        except Exception as e:
            logger.error(f"JAVDB æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return Result.fail(
                ServiceError(
                    ErrorCode.UNKNOWN_ERROR,
                    f"JAVDB æœå°‹ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}",
                    caused_by=e,
                )
            )

    def search_japanese_sites(
        self, code: str, stop_event: threading.Event
    ) -> Result[Dict]:
        """åªæœå°‹æ—¥æ–‡ç¶²ç«™ (AV-WIKI å’Œ chiba-f.net)"""
        if stop_event.is_set():
            return Result.fail(ServiceError(ErrorCode.VALIDATION_ERROR, "ä»»å‹™è¢«ä¸­æ­¢"))
        if code in self.search_cache:
            return Result.ok(self.search_cache[code])

        try:
            # ç¬¬ä¸€å±¤ï¼šAV-WIKI æœå°‹
            logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - AV-WIKI: {code}")
            result = self._search_av_wiki(code, stop_event)
            if result.success and result.data and result.data.get("actresses"):
                self.search_cache[code] = result.data
                return result

            # ç¬¬äºŒå±¤ï¼šchiba-f.net æœå°‹
            if not stop_event.is_set():
                logger.debug(f"ğŸ‡¯ğŸ‡µ æ—¥æ–‡ç¶²ç«™æœå°‹ - chiba-f.net: {code}")
                result = self._search_chiba_f_net(code, stop_event)
                if result.success and result.data and result.data.get("actresses"):
                    self.search_cache[code] = result.data
                    return result

            logger.warning(f"ç•ªè™Ÿ {code} æœªåœ¨æ—¥æ–‡ç¶²ç«™ä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚")
            return Result.fail(
                ServiceError(
                    ErrorCode.PARSING_ERROR, f"ç•ªè™Ÿ {code} æœªåœ¨æ—¥æ–‡ç¶²ç«™ä¸­æ‰¾åˆ°å¥³å„ªè³‡è¨Šã€‚"
                )
            )
        except Exception as e:
            logger.error(f"æ—¥æ–‡ç¶²ç«™æœå°‹ç•ªè™Ÿ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return Result.fail(
                ServiceError(
                    ErrorCode.UNKNOWN_ERROR,
                    f"æ—¥æ–‡ç¶²ç«™æœå°‹ç•ªè™Ÿ {code} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}",
                    caused_by=e,
                )
            )
