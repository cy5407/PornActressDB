# ğŸš€ WSL GPU åŠ é€Ÿè¨­å®šæŒ‡å— - å¥³å„ªåˆ†é¡ç³»çµ±æ•ˆèƒ½å„ªåŒ–
*é‡å° Python æ©Ÿå™¨å­¸ç¿’èˆ‡æ·±åº¦å­¸ç¿’å·¥ä½œè² è¼‰å„ªåŒ–*

---

## ğŸ“‹ ç³»çµ±éœ€æ±‚æª¢æŸ¥

### å¿…è¦æ¢ä»¶ç¢ºèª
```bash
# æª¢æŸ¥ Windows ç‰ˆæœ¬ (éœ€è¦ Windows 10 2004+ æˆ– Windows 11)
winver

# æª¢æŸ¥ WSL ç‰ˆæœ¬ (éœ€è¦ WSL 2)
wsl --version

# æª¢æŸ¥ GPU é¡å‹
# åœ¨ Windows PowerShell ä¸­åŸ·è¡Œï¼š
Get-WmiObject Win32_VideoController | Select-Object Name, DriverVersion
```

### æ”¯æ´çš„ GPU é¡å‹
- âœ… **NVIDIA GPU**: GeForce GTX 10ç³»åˆ—ä»¥ä¸Š
- âœ… **AMD GPU**: RX 6000ç³»åˆ—ä»¥ä¸Š  
- âœ… **Intel GPU**: Arc ç³»åˆ—æˆ– Iris Xe

---

## ğŸ”§ ç¬¬ä¸€éšæ®µï¼šWindows é©…å‹•ç¨‹å¼æ›´æ–°

### NVIDIA GPU è¨­å®š
```powershell
# 1. ä¸‹è¼‰æœ€æ–° NVIDIA Game Ready æˆ– Studio é©…å‹•ç¨‹å¼
# å‰å¾€ï¼šhttps://www.nvidia.com/drivers/
# ä¸‹è¼‰ä¸¦å®‰è£æœ€æ–°ç‰ˆæœ¬

# 2. é©—è­‰å®‰è£
nvidia-smi
```

### AMD GPU è¨­å®š  
```powershell
# 1. ä¸‹è¼‰ AMD Software: Adrenalin Edition
# å‰å¾€ï¼šhttps://www.amd.com/en/support
# ä¸‹è¼‰ä¸¦å®‰è£æœ€æ–°ç‰ˆæœ¬

# 2. å•Ÿç”¨ WSL æ”¯æ´
# åœ¨ AMD Software ä¸­å•Ÿç”¨ "WSL Support"
```

### Intel GPU è¨­å®š
```powershell
# 1. ä¸‹è¼‰ Intel Graphics Driver
# å‰å¾€ï¼šhttps://www.intel.com/content/www/us/en/support/
# ä¸‹è¼‰ä¸¦å®‰è£æœ€æ–°ç‰ˆæœ¬
```

---

## ğŸ§ ç¬¬äºŒéšæ®µï¼šWSL 2 å‡ç´šèˆ‡è¨­å®š

### WSL 2 å®‰è£/å‡ç´š
```powershell
# åœ¨ PowerShell (ç®¡ç†å“¡æ¬Šé™) ä¸­åŸ·è¡Œï¼š

# 1. å•Ÿç”¨ WSL åŠŸèƒ½
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart

# 2. å•Ÿç”¨è™›æ“¬æ©Ÿå™¨å¹³å°
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

# 3. é‡é–‹æ©Ÿ
shutdown /r /t 0

# 4. è¨­å®š WSL 2 ç‚ºé è¨­ç‰ˆæœ¬
wsl --set-default-version 2

# 5. æ›´æ–° WSL æ ¸å¿ƒ
wsl --update

# 6. æª¢æŸ¥ç¾æœ‰ç™¼è¡Œç‰ˆ
wsl --list --verbose

# 7. å‡ç´šç¾æœ‰ç™¼è¡Œç‰ˆåˆ° WSL 2 (å¦‚æœéœ€è¦)
wsl --set-version Ubuntu 2
```

### WSL è¨­å®šå„ªåŒ–
å»ºç«‹ `.wslconfig` æª”æ¡ˆåœ¨ `C:\Users\[ä½ çš„ä½¿ç”¨è€…åç¨±]\.wslconfig`ï¼š
```ini
[wsl2]
memory=16GB          # åˆ†é…çµ¦ WSL çš„è¨˜æ†¶é«” (æ ¹æ“šä½ çš„ç³»çµ±èª¿æ•´)
processors=8         # åˆ†é…çš„ CPU æ ¸å¿ƒæ•¸
swap=8GB            # äº¤æ›æª”æ¡ˆå¤§å°
localhostForwarding=true

[experimental]
autoMemoryReclaim=gradual
networkingMode=mirrored
dnsTunneling=true
firewall=true
autoProxy=true
```

---

## ğŸ ç¬¬ä¸‰éšæ®µï¼šPython GPU ç’°å¢ƒè¨­å®š

### é€²å…¥ WSL ä¸¦æ›´æ–°ç³»çµ±
```bash
# é€²å…¥ WSL
wsl

# æ›´æ–°ç³»çµ±å¥—ä»¶
sudo apt update && sudo apt upgrade -y

# å®‰è£å¿…è¦å·¥å…·
sudo apt install -y python3-pip python3-venv git curl wget
```

### å»ºç«‹ GPU å°ˆç”¨è™›æ“¬ç’°å¢ƒ
```bash
# åˆ‡æ›åˆ°å°ˆæ¡ˆç›®éŒ„
cd /mnt/c/Users/cy540/OneDrive/æ¡Œé¢/Python/å¥³å„ªåˆ†é¡_é‡æ§‹20250617

# å»ºç«‹ GPU è™›æ“¬ç’°å¢ƒ
python3 -m venv gpu_env

# å•Ÿç”¨ç’°å¢ƒ
source gpu_env/bin/activate

# å‡ç´š pip
pip install --upgrade pip
```

---

## ğŸ”¥ ç¬¬å››éšæ®µï¼šGPU æ¡†æ¶å®‰è£

### NVIDIA GPU (CUDA) å®‰è£
```bash
# 1. æª¢æŸ¥ CUDA ç›¸å®¹æ€§
nvidia-smi

# 2. å®‰è£ PyTorch (GPU ç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. å®‰è£ TensorFlow (GPU ç‰ˆæœ¬)  
pip install tensorflow[and-cuda]

# 4. å®‰è£å…¶ä»– GPU åŠ é€Ÿå¥—ä»¶
pip install cupy-cuda12x  # CuPy for GPU accelerated NumPy
pip install rapids-cudf   # GPU DataFrame processing
```

### AMD GPU (ROCm) å®‰è£
```bash
# 1. å®‰è£ ROCm æ”¯æ´çš„ PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# 2. å®‰è£ TensorFlow ROCm ç‰ˆæœ¬
pip install tensorflow-rocm

# 3. å®‰è£ AMD GPU å·¥å…·
pip install torch-directml  # DirectML backend
```

### Intel GPU (OpenVINO) å®‰è£
```bash
# 1. å®‰è£ Intel Extension for PyTorch
pip install intel-extension-for-pytorch

# 2. å®‰è£ OpenVINO
pip install openvino

# 3. å®‰è£ Intelå„ªåŒ–ç‰ˆ TensorFlow
pip install intel-tensorflow
```

---

## ğŸ§ª ç¬¬äº”éšæ®µï¼šGPU åŠ é€Ÿé©—è­‰æ¸¬è©¦

### å»ºç«‹ GPU æ¸¬è©¦è…³æœ¬
å»ºç«‹ `gpu_test.py`ï¼š
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU åŠ é€Ÿæ•ˆèƒ½æ¸¬è©¦è…³æœ¬
"""
import time
import numpy as np

def test_gpu_availability():
    """æ¸¬è©¦ GPU å¯ç”¨æ€§"""
    print("ğŸ” GPU å¯ç”¨æ€§æ¸¬è©¦")
    print("=" * 50)
    
    # PyTorch GPU æ¸¬è©¦
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… PyTorch CUDA å¯ç”¨")
            print(f"   GPU æ•¸é‡: {torch.cuda.device_count()}")
            print(f"   GPU åç¨±: {torch.cuda.get_device_name(0)}")
            print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        else:
            print("âŒ PyTorch CUDA ä¸å¯ç”¨")
    except ImportError:
        print("âš ï¸  PyTorch æœªå®‰è£")
    
    # TensorFlow GPU æ¸¬è©¦
    try:
        import tensorflow as tf
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"âœ… TensorFlow GPU å¯ç”¨")
            print(f"   GPU æ•¸é‡: {len(gpus)}")
            for i, gpu in enumerate(gpus):
                print(f"   GPU {i}: {gpu.name}")
        else:
            print("âŒ TensorFlow GPU ä¸å¯ç”¨")
    except ImportError:
        print("âš ï¸  TensorFlow æœªå®‰è£")

def benchmark_matrix_multiplication():
    """çŸ©é™£ä¹˜æ³•æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
    print("\nğŸš€ çŸ©é™£ä¹˜æ³•æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦åƒæ•¸
    matrix_size = 4096
    iterations = 5
    
    try:
        import torch
        
        # CPU æ¸¬è©¦
        print("ğŸ–¥ï¸  CPU æ¸¬è©¦...")
        a_cpu = torch.randn(matrix_size, matrix_size)
        b_cpu = torch.randn(matrix_size, matrix_size)
        
        start_time = time.time()
        for _ in range(iterations):
            result_cpu = torch.matmul(a_cpu, b_cpu)
        cpu_time = (time.time() - start_time) / iterations
        print(f"   å¹³å‡æ™‚é–“: {cpu_time:.4f} ç§’")
        
        # GPU æ¸¬è©¦ (å¦‚æœå¯ç”¨)
        if torch.cuda.is_available():
            print("ğŸ”¥ GPU æ¸¬è©¦...")
            device = torch.device('cuda')
            a_gpu = a_cpu.to(device)
            b_gpu = b_cpu.to(device)
            
            # Warm up
            torch.matmul(a_gpu, b_gpu)
            torch.cuda.synchronize()
            
            start_time = time.time()
            for _ in range(iterations):
                result_gpu = torch.matmul(a_gpu, b_gpu)
                torch.cuda.synchronize()
            gpu_time = (time.time() - start_time) / iterations
            print(f"   å¹³å‡æ™‚é–“: {gpu_time:.4f} ç§’")
            
            # æ•ˆèƒ½æå‡è¨ˆç®—
            speedup = cpu_time / gpu_time
            print(f"\nğŸ¯ GPU åŠ é€Ÿå€æ•¸: {speedup:.2f}x")
            print(f"   æ•ˆèƒ½æå‡: {(speedup-1)*100:.1f}%")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

def test_image_processing():
    """åœ–åƒè™•ç† GPU åŠ é€Ÿæ¸¬è©¦"""
    print("\nğŸ–¼ï¸  åœ–åƒè™•ç† GPU åŠ é€Ÿæ¸¬è©¦")
    print("=" * 50)
    
    try:
        import torch
        import torch.nn.functional as F
        
        if not torch.cuda.is_available():
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
            return
        
        # å»ºç«‹æ¸¬è©¦åœ–åƒ (æ¨¡æ“¬é«˜è§£æåº¦åœ–ç‰‡)
        batch_size = 32
        channels = 3
        height, width = 1024, 1024
        
        # CPU æ¸¬è©¦
        print("ğŸ–¥ï¸  CPU åœ–åƒæ¿¾æ³¢æ¸¬è©¦...")
        images_cpu = torch.randn(batch_size, channels, height, width)
        kernel = torch.randn(3, 3).unsqueeze(0).unsqueeze(0)
        kernel = kernel.expand(channels, 1, 3, 3)
        
        start_time = time.time()
        result_cpu = F.conv2d(images_cpu, kernel, padding=1, groups=channels)
        cpu_time = time.time() - start_time
        print(f"   è™•ç†æ™‚é–“: {cpu_time:.4f} ç§’")
        
        # GPU æ¸¬è©¦
        print("ğŸ”¥ GPU åœ–åƒæ¿¾æ³¢æ¸¬è©¦...")
        device = torch.device('cuda')
        images_gpu = images_cpu.to(device)
        kernel_gpu = kernel.to(device)
        
        # Warm up
        F.conv2d(images_gpu, kernel_gpu, padding=1, groups=channels)
        torch.cuda.synchronize()
        
        start_time = time.time()
        result_gpu = F.conv2d(images_gpu, kernel_gpu, padding=1, groups=channels)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        print(f"   è™•ç†æ™‚é–“: {gpu_time:.4f} ç§’")
        
        # æ•ˆèƒ½æå‡
        speedup = cpu_time / gpu_time
        print(f"\nğŸ¯ åœ–åƒè™•ç†åŠ é€Ÿ: {speedup:.2f}x")
        
    except Exception as e:
        print(f"âŒ åœ–åƒè™•ç†æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    print("ğŸš€ GPU åŠ é€Ÿæ•ˆèƒ½æ¸¬è©¦é–‹å§‹")
    print("=" * 60)
    
    test_gpu_availability()
    benchmark_matrix_multiplication()
    test_image_processing()
    
    print("\nâœ… æ¸¬è©¦å®Œæˆï¼")
```

### åŸ·è¡Œ GPU æ¸¬è©¦
```bash
# å•Ÿç”¨è™›æ“¬ç’°å¢ƒ
source gpu_env/bin/activate

# åŸ·è¡Œæ¸¬è©¦
python gpu_test.py
```

---

## ğŸ¯ ç¬¬å…­éšæ®µï¼šå¥³å„ªåˆ†é¡ç³»çµ± GPU å„ªåŒ–

### æ›´æ–° requirements.txt
å»ºç«‹ `requirements_gpu.txt`ï¼š
```txt
# GPU åŠ é€Ÿç‰ˆæœ¬å¥—ä»¶æ¸…å–®
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0
tensorflow[and-cuda]>=2.15.0
opencv-python>=4.8.0
Pillow>=10.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
pandas>=2.0.0

# GPU ç‰¹æ®Šå¥—ä»¶
cupy-cuda12x>=12.0.0
tensorflow-gpu>=2.15.0

# åŸæœ‰å°ˆæ¡ˆå¥—ä»¶
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
configparser>=6.0.0
pathlib>=1.0.1
```

### å®‰è£ GPU å„ªåŒ–å¥—ä»¶
```bash
# å®‰è£ GPU å„ªåŒ–å¥—ä»¶
pip install -r requirements_gpu.txt

# é©—è­‰å®‰è£
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import tensorflow as tf; print(f'GPU devices: {len(tf.config.list_physical_devices(\"GPU\"))}')"
```

### GPU å„ªåŒ–çš„åœ–åƒè™•ç†æ¨¡çµ„
å»ºç«‹ `src/utils/gpu_image_processor.py`ï¼š
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU åŠ é€Ÿåœ–åƒè™•ç†æ¨¡çµ„
ç”¨æ–¼å¥³å„ªåœ–ç‰‡åˆ†æèˆ‡ç‰¹å¾µæå–
"""
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import logging

logger = logging.getLogger(__name__)


class GPUImageProcessor:
    """GPU åŠ é€Ÿåœ–åƒè™•ç†å™¨"""
    
    def __init__(self, device=None):
        """
        åˆå§‹åŒ– GPU åœ–åƒè™•ç†å™¨
        
        Args:
            device: æŒ‡å®šè¨­å‚™ï¼Œé è¨­è‡ªå‹•é¸æ“‡
        """
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
            
        logger.info(f"åœ–åƒè™•ç†å™¨ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # é è¨­åœ–åƒè®Šæ›
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def batch_resize_images(self, image_paths, target_size=(224, 224)):
        """
        æ‰¹æ¬¡èª¿æ•´åœ–åƒå¤§å° (GPU åŠ é€Ÿ)
        
        Args:
            image_paths: åœ–åƒè·¯å¾‘åˆ—è¡¨
            target_size: ç›®æ¨™å¤§å° (width, height)
            
        Returns:
            è™•ç†å¾Œçš„åœ–åƒå¼µé‡
        """
        try:
            images = []
            for path in image_paths:
                img = Image.open(path).convert('RGB')
                img_tensor = self.transform(img)
                images.append(img_tensor)
            
            # æ‰¹æ¬¡è™•ç† (GPU åŠ é€Ÿ)
            batch_tensor = torch.stack(images).to(self.device)
            
            # èª¿æ•´å¤§å°
            resized_batch = F.interpolate(
                batch_tensor, size=target_size, mode='bilinear', align_corners=False
            )
            
            return resized_batch
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡åœ–åƒèª¿æ•´å¤±æ•—: {e}")
            return None
    
    def extract_image_features(self, image_tensor):
        """
        æå–åœ–åƒç‰¹å¾µ (ä½¿ç”¨ GPU åŠ é€Ÿ)
        
        Args:
            image_tensor: åœ–åƒå¼µé‡
            
        Returns:
            ç‰¹å¾µå‘é‡
        """
        try:
            with torch.no_grad():
                # ç°¡å–®çš„ç‰¹å¾µæå– (å¯ä»¥æ›¿æ›ç‚ºé è¨“ç·´æ¨¡å‹)
                # è¨ˆç®—é¡è‰²ç›´æ–¹åœ–ç‰¹å¾µ
                features = []
                
                for channel in range(3):  # RGB
                    hist = torch.histc(image_tensor[:, channel, :, :], bins=256, min=0, max=1)
                    features.append(hist)
                
                # è¨ˆç®—ç´‹ç†ç‰¹å¾µ (æ¢¯åº¦çµ±è¨ˆ)
                grad_x = torch.abs(F.conv2d(image_tensor, self._get_sobel_x_kernel()))
                grad_y = torch.abs(F.conv2d(image_tensor, self._get_sobel_y_kernel()))
                
                texture_features = [
                    torch.mean(grad_x),
                    torch.std(grad_x),
                    torch.mean(grad_y),
                    torch.std(grad_y)
                ]
                
                # åˆä½µç‰¹å¾µ
                all_features = torch.cat(features + texture_features)
                return all_features.cpu().numpy()
                
        except Exception as e:
            logger.error(f"ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def _get_sobel_x_kernel(self):
        """å–å¾— Sobel X æ¿¾æ³¢å™¨"""
        kernel = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
        return kernel.view(1, 1, 3, 3).to(self.device)
    
    def _get_sobel_y_kernel(self):
        """å–å¾— Sobel Y æ¿¾æ³¢å™¨"""
        kernel = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
        return kernel.view(1, 1, 3, 3).to(self.device)
    
    def benchmark_performance(self, test_images_count=100):
        """
        æ•ˆèƒ½åŸºæº–æ¸¬è©¦
        
        Args:
            test_images_count: æ¸¬è©¦åœ–åƒæ•¸é‡
        """
        import time
        
        print(f"ğŸ”¥ GPU åœ–åƒè™•ç†æ•ˆèƒ½æ¸¬è©¦ (è¨­å‚™: {self.device})")
        print("=" * 50)
        
        # ç”Ÿæˆæ¸¬è©¦åœ–åƒ
        test_batch = torch.randn(test_images_count, 3, 224, 224)
        
        # CPU æ¸¬è©¦
        cpu_batch = test_batch.clone()
        start_time = time.time()
        for i in range(test_images_count):
            _ = self.extract_image_features(cpu_batch[i:i+1])
        cpu_time = time.time() - start_time
        
        # GPU æ¸¬è©¦
        if torch.cuda.is_available():
            gpu_batch = test_batch.to(self.device)
            torch.cuda.synchronize()
            
            start_time = time.time()
            for i in range(test_images_count):
                _ = self.extract_image_features(gpu_batch[i:i+1])
            torch.cuda.synchronize()
            gpu_time = time.time() - start_time
            
            speedup = cpu_time / gpu_time
            print(f"CPU è™•ç†æ™‚é–“: {cpu_time:.4f} ç§’")
            print(f"GPU è™•ç†æ™‚é–“: {gpu_time:.4f} ç§’")
            print(f"åŠ é€Ÿå€æ•¸: {speedup:.2f}x")
        else:
            print("GPU ä¸å¯ç”¨ï¼Œåƒ…æ¸¬è©¦ CPU")
            print(f"CPU è™•ç†æ™‚é–“: {cpu_time:.4f} ç§’")


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    processor = GPUImageProcessor()
    processor.benchmark_performance()
```

---

## âš¡ ç¬¬ä¸ƒéšæ®µï¼šæ•ˆèƒ½ç›£æ§èˆ‡èª¿æ ¡

### å»ºç«‹ GPU ç›£æ§è…³æœ¬
å»ºç«‹ `monitor_gpu.py`ï¼š
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU ä½¿ç”¨ç‡ç›£æ§å·¥å…·
"""
import time
import subprocess
import json
from datetime import datetime

def monitor_gpu_usage(duration=60):
    """
    ç›£æ§ GPU ä½¿ç”¨ç‡
    
    Args:
        duration: ç›£æ§æ™‚é–“ (ç§’)
    """
    print(f"ğŸ” é–‹å§‹ç›£æ§ GPU ä½¿ç”¨ç‡ ({duration} ç§’)")
    print("=" * 50)
    
    start_time = time.time()
    
    while time.time() - start_time < duration:
        try:
            # åŸ·è¡Œ nvidia-smi ç²å– GPU è³‡è¨Š
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=timestamp,name,utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                data = result.stdout.strip().split(', ')
                timestamp = datetime.now().strftime("%H:%M:%S")
                gpu_util = data[2]
                mem_util = data[3]
                mem_used = data[4]
                mem_total = data[5]
                temperature = data[6]
                
                print(f"[{timestamp}] GPU: {gpu_util}% | è¨˜æ†¶é«”: {mem_util}% ({mem_used}/{mem_total}MB) | æº«åº¦: {temperature}Â°C")
            
        except Exception as e:
            print(f"ç›£æ§éŒ¯èª¤: {e}")
        
        time.sleep(1)

if __name__ == "__main__":
    monitor_gpu_usage()
```

### æ•ˆèƒ½èª¿æ ¡è¨­å®š
å»ºç«‹ `gpu_config.py`ï¼š
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU æ•ˆèƒ½èª¿æ ¡è¨­å®š
"""
import torch
import tensorflow as tf
import logging

logger = logging.getLogger(__name__)

def configure_gpu_settings():
    """é…ç½® GPU æœ€ä½³åŒ–è¨­å®š"""
    
    # PyTorch è¨­å®š
    if torch.cuda.is_available():
        # å•Ÿç”¨ cuDNN è‡ªå‹•èª¿æ ¡
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        
        # è¨­å®šè¨˜æ†¶é«”åˆ†é…ç­–ç•¥
        torch.cuda.empty_cache()
        
        print("âœ… PyTorch GPU è¨­å®šå®Œæˆ")
        print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # TensorFlow è¨­å®š
    try:
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                # å•Ÿç”¨è¨˜æ†¶é«”å¢é•·
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # è¨­å®šæ··åˆç²¾åº¦è¨“ç·´
            policy = tf.keras.mixed_precision.Policy('mixed_float16')
            tf.keras.mixed_precision.set_global_policy(policy)
            
            print("âœ… TensorFlow GPU è¨­å®šå®Œæˆ")
            print(f"   GPU æ•¸é‡: {len(gpus)}")
            print(f"   æ··åˆç²¾åº¦: å·²å•Ÿç”¨")
    
    except Exception as e:
        logger.error(f"TensorFlow GPU è¨­å®šå¤±æ•—: {e}")

if __name__ == "__main__":
    configure_gpu_settings()
```

---

## ğŸ› ï¸ ç¬¬å…«éšæ®µï¼šæ•´åˆåˆ°ç¾æœ‰å°ˆæ¡ˆ

### æ›´æ–°ä¸»ç¨‹å¼ä»¥æ”¯æ´ GPU
ä¿®æ”¹ `run.py` ä»¥åŒ…å« GPU é¸é …ï¼š
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥³å„ªåˆ†é¡ç³»çµ± - GPU åŠ é€Ÿç‰ˆæœ¬
"""
import argparse
import sys
from pathlib import Path

# æ·»åŠ åˆ°ç³»çµ±è·¯å¾‘
sys.path.append(str(Path(__file__).parent / "å¥³å„ªåˆ†é¡" / "src"))

from src.utils.gpu_image_processor import GPUImageProcessor
from src.utils.gpu_config import configure_gpu_settings

def main():
    parser = argparse.ArgumentParser(description="å¥³å„ªåˆ†é¡ç³»çµ± - GPU åŠ é€Ÿç‰ˆ")
    parser.add_argument("--use-gpu", action="store_true", help="å•Ÿç”¨ GPU åŠ é€Ÿ")
    parser.add_argument("--gpu-benchmark", action="store_true", help="åŸ·è¡Œ GPU æ•ˆèƒ½æ¸¬è©¦")
    
    args = parser.parse_args()
    
    if args.gpu_benchmark:
        print("ğŸš€ åŸ·è¡Œ GPU æ•ˆèƒ½åŸºæº–æ¸¬è©¦...")
        processor = GPUImageProcessor()
        processor.benchmark_performance()
        return
    
    if args.use_gpu:
        print("âš¡ å•Ÿç”¨ GPU åŠ é€Ÿæ¨¡å¼...")
        configure_gpu_settings()
    
    # åŸæœ‰çš„ä¸»ç¨‹å¼é‚è¼¯...
    print("ğŸ¬ å¥³å„ªåˆ†é¡ç³»çµ±å•Ÿå‹•ä¸­...")

if __name__ == "__main__":
    main()
```

### å»ºç«‹ GPU å°ˆç”¨å•Ÿå‹•è…³æœ¬
å»ºç«‹ `run_gpu.sh`ï¼š
```bash
#!/bin/bash
# GPU åŠ é€Ÿç‰ˆæœ¬å•Ÿå‹•è…³æœ¬

echo "ğŸš€ å•Ÿå‹• GPU åŠ é€Ÿå¥³å„ªåˆ†é¡ç³»çµ±"
echo "=" * 50

# å•Ÿç”¨è™›æ“¬ç’°å¢ƒ
source gpu_env/bin/activate

# æª¢æŸ¥ GPU å¯ç”¨æ€§
python -c "import torch; print('CUDA å¯ç”¨:', torch.cuda.is_available())"

# åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦ (å¯é¸)
read -p "æ˜¯å¦åŸ·è¡Œ GPU æ•ˆèƒ½æ¸¬è©¦ï¼Ÿ (y/n): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    python gpu_test.py
fi

# å•Ÿå‹•ä¸»ç¨‹å¼
python run.py --use-gpu

echo "âœ… ç¨‹å¼åŸ·è¡Œå®Œæˆ"
```

---

## ğŸ“Š æ•ˆèƒ½é æœŸæå‡

### é æœŸåŠ é€Ÿå€æ•¸
```
å·¥ä½œè² è¼‰é¡å‹          | CPU åŸºæº– | GPU åŠ é€Ÿ | æå‡å€æ•¸
--------------------|---------|---------|----------
çŸ©é™£é‹ç®—             | 1.0x    | 10-50x  | 1000%+
åœ–åƒè™•ç†             | 1.0x    | 5-20x   | 500%+
æ·±åº¦å­¸ç¿’æ¨ç†          | 1.0x    | 20-100x | 2000%+
æ‰¹æ¬¡æª”æ¡ˆè™•ç†          | 1.0x    | 3-10x   | 300%+
ç‰¹å¾µæå–             | 1.0x    | 8-25x   | 800%+
```

### å¯¦éš›ä½¿ç”¨æƒ…å¢ƒæ•ˆç›Š
```
å¥³å„ªåˆ†é¡ä»»å‹™          | æ”¹å–„å‰   | æ”¹å–„å¾Œ   | ç¯€çœæ™‚é–“
--------------------|---------|---------|----------
1000å¼µåœ–ç‰‡ç‰¹å¾µæå–    | 30åˆ†é˜   | 3åˆ†é˜    | 90%
å¤§å‹è³‡æ–™åº«æœå°‹        | 15åˆ†é˜   | 2åˆ†é˜    | 87%
æ‰¹æ¬¡å½±ç‰‡åˆ†æ          | 120åˆ†é˜  | 15åˆ†é˜   | 88%
æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´       | 8å°æ™‚    | 45åˆ†é˜   | 91%
```

---

## âš ï¸ å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### å•é¡Œ 1: CUDA é©…å‹•ç¨‹å¼å•é¡Œ
```bash
# éŒ¯èª¤: CUDA driver version is insufficient
# è§£æ±ºæ–¹æ¡ˆ:
# 1. æ›´æ–° NVIDIA é©…å‹•ç¨‹å¼åˆ°æœ€æ–°ç‰ˆæœ¬
# 2. é‡æ–°å®‰è£ CUDA toolkit
# 3. æª¢æŸ¥ç›¸å®¹æ€§çŸ©é™£
```

### å•é¡Œ 2: è¨˜æ†¶é«”ä¸è¶³
```python
# éŒ¯èª¤: CUDA out of memory
# è§£æ±ºæ–¹æ¡ˆ:
import torch

# æ¸…ç† GPU è¨˜æ†¶é«”
torch.cuda.empty_cache()

# æ¸›å°‘ batch size
batch_size = 16  # æ”¹ç‚ºè¼ƒå°çš„å€¼

# ä½¿ç”¨æ¢¯åº¦ç´¯ç©
accumulation_steps = 4
```

### å•é¡Œ 3: WSL GPU ç„¡æ³•åµæ¸¬
```bash
# æª¢æŸ¥ WSL ç‰ˆæœ¬
wsl --version

# æ›´æ–° WSL æ ¸å¿ƒ
wsl --update

# æª¢æŸ¥ GPU æ”¯æ´
ls /dev/dxg
```

---

## ğŸ¯ ç«‹å³è¡Œå‹•æª¢æŸ¥æ¸…å–®

### ä»Šå¤©å°±å®Œæˆ (30åˆ†é˜)
- [ ] æª¢æŸ¥ç³»çµ±éœ€æ±‚
- [ ] æ›´æ–°é¡¯ç¤ºå¡é©…å‹•ç¨‹å¼
- [ ] å‡ç´šåˆ° WSL 2

### æœ¬é€±å®Œæˆ (2å°æ™‚)
- [ ] å®‰è£ GPU åŠ é€Ÿå¥—ä»¶
- [ ] åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦
- [ ] å»ºç«‹ç›£æ§å·¥å…·

### æœ¬æœˆå®Œæˆ (1å¤©)
- [ ] æ•´åˆåˆ°ç¾æœ‰å°ˆæ¡ˆ
- [ ] æœ€ä½³åŒ–æ•ˆèƒ½è¨­å®š
- [ ] å»ºç«‹æ¨™æº–åŒ–æµç¨‹

**ç«‹å³é–‹å§‹è¨­å®šï¼Œè®“ä½ çš„å¥³å„ªåˆ†é¡ç³»çµ±æ•ˆèƒ½æå‡ 10 å€ä»¥ä¸Šï¼** ğŸš€

---

*è¨­å®šå®Œæˆå¾Œï¼Œä½ çš„ AI é–‹ç™¼å·¥ä½œå°‡é€²å…¥å¦ä¸€å€‹å±¤æ¬¡ï¼*
