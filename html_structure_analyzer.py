# -*- coding: utf-8 -*-
"""
HTML çµæ§‹è§£ææ¸¬è©¦è…³æœ¬
ç”¨æ–¼åˆ†æ av-wiki.net å’Œ chiba-f.net çš„é é¢çµæ§‹å’Œç·¨ç¢¼å•é¡Œ
"""

import requests
import httpx
from bs4 import BeautifulSoup
import chardet
import logging
from typing import Dict, Any
import asyncio

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HTMLStructureAnalyzer:
    """HTML çµæ§‹åˆ†æå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8,ja;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def analyze_encoding(self, content: bytes, url: str) -> Dict[str, Any]:
        """åˆ†æå…§å®¹ç·¨ç¢¼"""
        try:
            # ä½¿ç”¨ chardet æª¢æ¸¬ç·¨ç¢¼
            detected = chardet.detect(content)
            logger.info(f"ğŸ” [{url}] æª¢æ¸¬åˆ°çš„ç·¨ç¢¼: {detected}")
            
            # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼æ–¹å¼è§£ç¢¼
            encodings_to_try = [
                detected.get('encoding', 'utf-8'),
                'utf-8',
                'shift_jis',
                'euc-jp',
                'iso-2022-jp',
                'cp932',
                'gb2312',
                'big5'
            ]
            
            results = {}
            for encoding in encodings_to_try:
                if encoding:
                    try:
                        decoded = content.decode(encoding, errors='replace')
                        results[encoding] = {
                            'success': True,
                            'length': len(decoded),
                            'has_replacement_chars': 'ï¿½' in decoded,
                            'replacement_count': decoded.count('ï¿½')
                        }
                        logger.info(f"âœ… [{url}] {encoding} è§£ç¢¼æˆåŠŸï¼Œæ›¿æ›å­—ç¬¦æ•¸: {decoded.count('ï¿½')}")
                    except Exception as e:
                        results[encoding] = {'success': False, 'error': str(e)}
                        logger.warning(f"âŒ [{url}] {encoding} è§£ç¢¼å¤±æ•—: {e}")
            
            return {
                'detected_encoding': detected,
                'encoding_results': results,
                'content_length': len(content)
            }
        except Exception as e:
            logger.error(f"âŒ [{url}] ç·¨ç¢¼åˆ†æå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def analyze_html_structure(self, html_content: str, url: str) -> Dict[str, Any]:
        """åˆ†æHTMLçµæ§‹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # åŸºæœ¬çµæ§‹åˆ†æ
            structure = {
                'title': soup.title.string if soup.title else 'No title',
                'meta_charset': None,
                'search_results': [],
                'actress_info': [],
                'studio_info': [],
                'video_info': []
            }
            
            # æª¢æŸ¥é é¢ç·¨ç¢¼è²æ˜
            charset_meta = soup.find('meta', {'charset': True})
            if charset_meta:
                structure['meta_charset'] = charset_meta.get('charset')
            else:
                content_type_meta = soup.find('meta', {'http-equiv': 'Content-Type'})
                if content_type_meta:
                    content = content_type_meta.get('content', '')
                    if 'charset=' in content:
                        structure['meta_charset'] = content.split('charset=')[1].strip()
            
            # é‡å°ä¸åŒç¶²ç«™åˆ†æç‰¹å®šçµæ§‹
            if 'av-wiki.net' in url:
                structure.update(self._analyze_avwiki_structure(soup))
            elif 'chiba-f.net' in url:
                structure.update(self._analyze_chibaf_structure(soup))
            
            return structure
        except Exception as e:
            logger.error(f"âŒ [{url}] HTMLçµæ§‹åˆ†æå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _analyze_avwiki_structure(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """åˆ†æ av-wiki.net çš„HTMLçµæ§‹"""
        result = {}
        
        # æœå°‹çµæœ
        search_results = soup.find_all('div', class_='product-item') or soup.find_all('article')
        result['search_results_count'] = len(search_results)
        
        if search_results:
            for item in search_results[:3]:  # åªåˆ†æå‰3å€‹çµæœ
                item_info = {
                    'title': '',
                    'actress': '',
                    'studio': '',
                    'code': ''
                }
                
                # å˜—è©¦æå–æ¨™é¡Œ
                title_elem = item.find('h2') or item.find('h3') or item.find('a')
                if title_elem:
                    item_info['title'] = title_elem.get_text(strip=True)
                
                # å˜—è©¦æå–å…¶ä»–è³‡è¨Š
                links = item.find_all('a')
                for link in links:
                    text = link.get_text(strip=True)
                    href = link.get('href', '')
                    if any(actress_keyword in href.lower() for actress_keyword in ['actress', 'performer', 'star']):
                        item_info['actress'] = text
                    elif any(studio_keyword in href.lower() for studio_keyword in ['studio', 'maker', 'label']):
                        item_info['studio'] = text
                
                result.setdefault('search_items', []).append(item_info)
        
        return result
    
    def _analyze_chibaf_structure(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """åˆ†æ chiba-f.net çš„HTMLçµæ§‹"""
        result = {}
        
        # æœå°‹çµæœ
        search_results = soup.find_all('div', class_='search-result') or soup.find_all('div', class_='item')
        result['search_results_count'] = len(search_results)
        
        if search_results:
            for item in search_results[:3]:  # åªåˆ†æå‰3å€‹çµæœ
                item_info = {
                    'title': '',
                    'actress': '',
                    'studio': '',
                    'code': ''
                }
                
                # å˜—è©¦æå–æ¨™é¡Œ
                title_elem = item.find('h2') or item.find('h3') or item.find('a')
                if title_elem:
                    item_info['title'] = title_elem.get_text(strip=True)
                
                result.setdefault('search_items', []).append(item_info)
        
        return result
    
    def test_url_sync(self, url: str) -> Dict[str, Any]:
        """åŒæ­¥æ¸¬è©¦URL"""
        logger.info(f"ğŸŒ é–‹å§‹åŒæ­¥æ¸¬è©¦: {url}")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # ç·¨ç¢¼åˆ†æ
            encoding_analysis = self.analyze_encoding(response.content, url)
            
            # ä½¿ç”¨æœ€ä½³ç·¨ç¢¼è§£ç¢¼å…§å®¹
            best_encoding = 'utf-8'
            if encoding_analysis.get('encoding_results'):
                for enc, result in encoding_analysis['encoding_results'].items():
                    if result.get('success') and result.get('replacement_count', float('inf')) == 0:
                        best_encoding = enc
                        break
            
            try:
                html_content = response.content.decode(best_encoding, errors='replace')
            except:
                html_content = response.text
            
            # HTMLçµæ§‹åˆ†æ
            structure_analysis = self.analyze_html_structure(html_content, url)
            
            return {
                'url': url,
                'status_code': response.status_code,
                'encoding_analysis': encoding_analysis,
                'structure_analysis': structure_analysis,
                'response_headers': dict(response.headers),
                'content_preview': html_content[:1000] + '...' if len(html_content) > 1000 else html_content
            }
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥æ¸¬è©¦å¤±æ•— [{url}]: {e}")
            return {'url': url, 'error': str(e)}
    
    async def test_url_async(self, url: str) -> Dict[str, Any]:
        """éåŒæ­¥æ¸¬è©¦URL"""
        logger.info(f"ğŸŒ é–‹å§‹éåŒæ­¥æ¸¬è©¦: {url}")
        
        try:
            async with httpx.AsyncClient(headers=self.headers, timeout=10) as client:
                response = await client.get(url)
                response.raise_for_status()
                
                # ç·¨ç¢¼åˆ†æ
                encoding_analysis = self.analyze_encoding(response.content, url)
                
                # ä½¿ç”¨æœ€ä½³ç·¨ç¢¼è§£ç¢¼å…§å®¹
                best_encoding = 'utf-8'
                if encoding_analysis.get('encoding_results'):
                    for enc, result in encoding_analysis['encoding_results'].items():
                        if result.get('success') and result.get('replacement_count', float('inf')) == 0:
                            best_encoding = enc
                            break
                
                try:
                    html_content = response.content.decode(best_encoding, errors='replace')
                except:
                    html_content = response.text
                
                # HTMLçµæ§‹åˆ†æ
                structure_analysis = self.analyze_html_structure(html_content, url)
                
                return {
                    'url': url,
                    'status_code': response.status_code,
                    'encoding_analysis': encoding_analysis,
                    'structure_analysis': structure_analysis,
                    'response_headers': dict(response.headers),
                    'content_preview': html_content[:1000] + '...' if len(html_content) > 1000 else html_content
                }
                
        except Exception as e:
            logger.error(f"âŒ éåŒæ­¥æ¸¬è©¦å¤±æ•— [{url}]: {e}")
            return {'url': url, 'error': str(e)}

def run_tests():
    """åŸ·è¡Œæ¸¬è©¦"""
    analyzer = HTMLStructureAnalyzer()
    
    test_urls = [
        'https://av-wiki.net/?s=MIDV-661&post_type=product',
        'https://chiba-f.net/search/?keyword=MIDV-661'
    ]
    
    logger.info("ğŸ¯ é–‹å§‹HTMLçµæ§‹å’Œç·¨ç¢¼åˆ†ææ¸¬è©¦")
    logger.info("=" * 60)
    
    # åŒæ­¥æ¸¬è©¦
    logger.info("ğŸ“Š åŒæ­¥æ¸¬è©¦çµæœ:")
    for url in test_urls:
        result = analyzer.test_url_sync(url)
        print_analysis_result(result)
    
    # éåŒæ­¥æ¸¬è©¦
    logger.info("ğŸ“Š éåŒæ­¥æ¸¬è©¦çµæœ:")
    
    async def run_async_tests():
        tasks = [analyzer.test_url_async(url) for url in test_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"âŒ éåŒæ­¥æ¸¬è©¦ç•°å¸¸: {result}")
            else:
                print_analysis_result(result)
    
    asyncio.run(run_async_tests())

def print_analysis_result(result: Dict[str, Any]):
    """å°å‡ºåˆ†æçµæœ"""
    if 'error' in result:
        logger.error(f"âŒ {result['url']}: {result['error']}")
        return
    
    logger.info(f"\nğŸ” åˆ†æçµæœ: {result['url']}")
    logger.info(f"   ç‹€æ…‹ç¢¼: {result['status_code']}")
    
    # ç·¨ç¢¼åˆ†æçµæœ
    encoding_analysis = result.get('encoding_analysis', {})
    if 'detected_encoding' in encoding_analysis:
        detected = encoding_analysis['detected_encoding']
        logger.info(f"   æª¢æ¸¬ç·¨ç¢¼: {detected.get('encoding')} (ä¿¡å¿ƒåº¦: {detected.get('confidence', 0):.2f})")
    
    if 'encoding_results' in encoding_analysis:
        successful_encodings = [
            enc for enc, res in encoding_analysis['encoding_results'].items() 
            if res.get('success') and res.get('replacement_count', 0) == 0
        ]
        if successful_encodings:
            logger.info(f"   æˆåŠŸç·¨ç¢¼: {', '.join(successful_encodings)}")
        else:
            logger.warning("   âš ï¸ æ²’æœ‰å®Œç¾çš„ç·¨ç¢¼æ–¹å¼")
    
    # HTMLçµæ§‹åˆ†æçµæœ
    structure = result.get('structure_analysis', {})
    if 'title' in structure:
        logger.info(f"   é é¢æ¨™é¡Œ: {structure['title']}")
    if 'meta_charset' in structure:
        logger.info(f"   è²æ˜ç·¨ç¢¼: {structure['meta_charset']}")
    if 'search_results_count' in structure:
        logger.info(f"   æœå°‹çµæœæ•¸: {structure['search_results_count']}")
    
    if 'search_items' in structure:
        logger.info("   æœå°‹é …ç›®:")
        for i, item in enumerate(structure['search_items'][:2]):
            logger.info(f"     [{i+1}] æ¨™é¡Œ: {item.get('title', 'N/A')}")
            if item.get('actress'):
                logger.info(f"         å¥³å„ª: {item['actress']}")
            if item.get('studio'):
                logger.info(f"         ç‰‡å•†: {item['studio']}")

if __name__ == "__main__":
    run_tests()
