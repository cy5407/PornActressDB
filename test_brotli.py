#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ Brotli è§£å£“åŠŸèƒ½
"""

import httpx
import brotli
from bs4 import BeautifulSoup
from urllib.parse import quote

def test_brotli_decompression():
    """æ¸¬è©¦ Brotli è§£å£“åŠŸèƒ½"""
    print("ğŸš€ æ¸¬è©¦ Brotli è§£å£“åŠŸèƒ½")
    print("=" * 60)
    
    test_urls = [
        ("AV-WIKI SONE-553", "https://av-wiki.net/?s=SONE-553&post_type=product"),
        ("chiba-f.net SONE-553", "https://chiba-f.net/search/?keyword=SONE-553"),
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    
    with httpx.Client(timeout=30, follow_redirects=True) as client:
        for test_name, url in test_urls:
            print(f"\nğŸ” æ¸¬è©¦: {test_name}")
            print(f"ğŸ“‹ URL: {url}")
            print("-" * 60)
            
            try:
                response = client.get(url, headers=headers)
                print(f"ğŸ“Š å›æ‡‰ç‹€æ…‹: {response.status_code}")
                print(f"ğŸ“ å£“ç¸®å…§å®¹é•·åº¦: {len(response.content)} bytes")
                
                content_encoding = response.headers.get('content-encoding', 'none')
                print(f"ğŸ—œï¸ å…§å®¹ç·¨ç¢¼: {content_encoding}")
                
                # å˜—è©¦ Brotli è§£å£“
                if content_encoding == 'br':
                    print("âœ¨ å˜—è©¦ Brotli è§£å£“...")
                    try:
                        decompressed = brotli.decompress(response.content)
                        print(f"ğŸ“ è§£å£“å¾Œé•·åº¦: {len(decompressed)} bytes")
                        
                        # å˜—è©¦ UTF-8 è§£ç¢¼
                        text_content = decompressed.decode('utf-8', errors='replace')
                        replacement_ratio = text_content.count('\ufffd') / len(text_content) if text_content else 1.0
                        print(f"ğŸ“ UTF-8 è§£ç¢¼æ›¿æ›æ¯”ä¾‹: {replacement_ratio:.3f}")
                        
                        # æª¢æŸ¥ HTML çµæ§‹
                        if '<html' in text_content.lower() and '</html>' in text_content.lower():
                            print("âœ… åŒ…å«å®Œæ•´ HTML çµæ§‹")
                            
                            # è§£æ HTML
                            soup = BeautifulSoup(text_content, 'html.parser')
                            print(f"ğŸ“„ è§£æåˆ° {len(soup.find_all())} å€‹ HTML æ¨™ç±¤")
                            
                            # æª¢æŸ¥æœŸæœ›çš„å…ƒç´ 
                            if 'av-wiki' in url:
                                actress_elements = soup.find_all(class_="actress-name")
                                print(f"ğŸ­ æ‰¾åˆ° {len(actress_elements)} å€‹ actress-name å…ƒç´ ")
                                
                                # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦å…¶ä»–å¯èƒ½çš„é¸æ“‡å™¨
                                if not actress_elements:
                                    # å˜—è©¦æ‰¾åŒ…å«å¥³å„ªç›¸é—œçš„å…ƒç´ 
                                    potential_elements = []
                                    for selector in ['a[href*="actress"]', '.performer', '.cast', '[class*="actress"]']:
                                        elements = soup.select(selector)
                                        if elements:
                                            potential_elements.extend(elements)
                                    print(f"ğŸ” æ‰¾åˆ° {len(potential_elements)} å€‹æ½›åœ¨å¥³å„ªå…ƒç´ ")
                                    
                                    # é¡¯ç¤ºå‰å¹¾å€‹å…ƒç´ çš„å…§å®¹
                                    for i, elem in enumerate(potential_elements[:3]):
                                        text = elem.get_text().strip()
                                        print(f"    {i+1}. {text[:50]}...")
                                        
                            elif 'chiba-f' in url:
                                product_divs = soup.find_all('div', class_='product-div')
                                print(f"ğŸ“¦ æ‰¾åˆ° {len(product_divs)} å€‹ product-div å…ƒç´ ")
                                
                                # å˜—è©¦æ‰¾å…¶ä»–ç”¢å“ç›¸é—œå…ƒç´ 
                                if not product_divs:
                                    for selector in ['.product', '.item', '.card', '[class*="product"]']:
                                        elements = soup.select(selector)
                                        if elements:
                                            print(f"ğŸ” æ‰¾åˆ° {len(elements)} å€‹ {selector} å…ƒç´ ")
                            
                            # é¡¯ç¤ºé é¢æ¨™é¡Œå’Œä¸»è¦å…§å®¹çµæ§‹
                            title = soup.find('title')
                            if title:
                                print(f"ğŸ“‹ é é¢æ¨™é¡Œ: {title.get_text().strip()}")
                            
                            # é¡¯ç¤ºä¸»è¦å…§å®¹å€åŸŸ
                            main_content = soup.find('main') or soup.find('div', id='main') or soup.find('div', class_='content')
                            if main_content:
                                content_text = main_content.get_text().strip()[:200]
                                print(f"ğŸ“– ä¸»è¦å…§å®¹é è¦½: {content_text}...")
                                
                        else:
                            print("âŒ ä¸åŒ…å«å®Œæ•´ HTML çµæ§‹")
                            preview = text_content[:300].replace('\n', ' ')
                            print(f"ğŸ“„ å…§å®¹é è¦½: {preview}...")
                            
                    except Exception as e:
                        print(f"âŒ Brotli è§£å£“å¤±æ•—: {e}")
                        
                else:
                    print(f"â„¹ï¸ ä¸æ˜¯ Brotli ç·¨ç¢¼ï¼Œæ˜¯: {content_encoding}")
                    
            except Exception as e:
                print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")
            
            print("\nâ° ç­‰å¾… 2 ç§’...")
            import time
            time.sleep(2)

if __name__ == "__main__":
    test_brotli_decompression()
